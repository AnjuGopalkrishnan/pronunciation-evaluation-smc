{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing required packages"
   ],
   "metadata": {
    "id": "kJErFaz_Eib4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWkjs_8PHcBf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726462440,
     "user_tz": -480,
     "elapsed": 6937,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    },
    "outputId": "93e5043a-5afb-4257-834e-2f7adb8622b6"
   },
   "outputs": [],
   "source": [
    "!pip install speechbrain\n",
    "!pip install textgrid transformers librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing requried modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import librosa\n",
    "import csv\n",
    "from google.colab import drive, files\n",
    "from io import StringIO\n",
    "from speechbrain.utils.metric_stats import ErrorRateStats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mounting google drive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiA9SjWIHOx5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726500085,
     "user_tz": -480,
     "elapsed": 25606,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1c6493b4-3791-4d22-c3b4-8821c417a2ec"
   },
   "outputs": [],
   "source": [
    "# Mount drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Constants & Importing Eval metric file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sikO-MV-Iv0V",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726528101,
     "user_tz": -480,
     "elapsed": 441,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)folder_path = '/content/drive/MyDrive/CS5647_Project'\n",
    "os.chdir(folder_path)\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory after change:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQkBQKicOBmT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726531569,
     "user_tz": -480,
     "elapsed": 1169,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    }
   },
   "outputs": [],
   "source": [
    "from mpd_eval_v3 import MpdStats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the speech brain object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3meQIqYI1tr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726538068,
     "user_tz": -480,
     "elapsed": 428,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    }
   },
   "outputs": [],
   "source": [
    "def make_attn_mask(wavs, wav_lens):\n",
    "    \"\"\"\n",
    "    wav_lens: relative lengths(i.e. 0-1) of a batch. shape: (bs, )\n",
    "    return a tensor of shape (bs, seq_len), representing mask on allowed positions.\n",
    "            1 for regular tokens, 0 for padded tokens\n",
    "    \"\"\"\n",
    "    abs_lens = (wav_lens*wavs.shape[1]).long()\n",
    "    attn_mask = wavs.new(wavs.shape).zero_().long()\n",
    "    for i in range(len(abs_lens)):\n",
    "        attn_mask[i, :abs_lens[i]] = 1\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRjl6kGqI7Lr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726540931,
     "user_tz": -480,
     "elapsed": 753,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    }
   },
   "outputs": [],
   "source": [
    "class ASR(sb.Brain):\n",
    "    # Inference code\n",
    "        #Testing for one single audio file\n",
    "    def compute_forward_evaluate(self, batch, stage):\n",
    "        \"Given an input batch it computes the phoneme probabilities.\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs = batch\n",
    "\n",
    "        #creating a dummy wav_lens with shape of [batch,1] with 1\n",
    "        wav_lens = torch.ones((1,1)).to(self.device)\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        attn_mask = None\n",
    "        feats = self.modules.wav2vec2(wavs)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "        # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n",
    "        # that is, it return a list of list with different lengths\n",
    "        sequence = sb.decoders.ctc_greedy_decode(\n",
    "            p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "        )\n",
    "        transcriptions = [\" \".join(self.label_encoder.decode_ndim(s)) for s in sequence]\n",
    "        return transcriptions, sequence\n",
    "\n",
    "    def prepare_test_audio_for_inference(self, test_audio_path):\n",
    "        # Use wav2vec processor to do normalization\n",
    "        audio_signal, _ = librosa.core.load(test_audio_path, sr=self.hparams.sample_rate)\n",
    "        sig = self.hparams.wav2vec2.feature_extractor(\n",
    "            audio_signal,\n",
    "            sampling_rate=self.hparams.sample_rate,\n",
    "        ).input_values #since its only 1 file not taking [0]\n",
    "\n",
    "        sig = torch.Tensor(sig)\n",
    "        return sig\n",
    "\n",
    "    def get_predicted_phonemes_for_test_audio(self, test_audio_path):\n",
    "        print(f\"Using librosa to load the audio\")\n",
    "        batch = self.prepare_test_audio_for_inference(test_audio_path)\n",
    "        print(f\"Loading the best model & setting to eval mode\")\n",
    "        self.on_evaluate_start(min_key=\"PER\") # We call the on_evaluate_start that will load the best model\n",
    "        self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "        self.modules.wav2vec2.model.config.apply_spec_augment = False  # make sure no spec aug applied on wav2vec2\n",
    "\n",
    "        with torch.no_grad():\n",
    "          print(\"Making predictions from the best model\")\n",
    "          preds, seq = self.compute_forward_evaluate(batch, stage=sb.Stage.TEST)\n",
    "          print(\"Got the predictions\")\n",
    "        return preds, seq\n",
    "\n",
    "    def evaluate_test_audio(self, test_audio_path, canonical_phonemes):\n",
    "        predicted_phonemes, predicted_sequence = self.get_predicted_phonemes_for_test_audio(test_audio_path)\n",
    "        predicted_phonemes = predicted_phonemes.split()\n",
    "        predicted_sequence_without_sil = [[]]\n",
    "\n",
    "        # Ensure that we remove the sils\n",
    "        for pred_phoneme, pred_seq in zip(predicted_phonemes, predicted_sequence[0]):\n",
    "            if pred_phoneme != \"sil\":\n",
    "                predicted_sequence_without_sil[0].append(pred_seq)\n",
    "\n",
    "\n",
    "        print(\"Converting canonical to appropriate format for getting error\")\n",
    "        phn_list_canonical = canonical_phonemes.strip().split()\n",
    "        phn_list_canonical_without_sil = list(filter(lambda phn: phn != \"sil\", phn_list_canonical))\n",
    "        phn_encoded_list_canonical = [self.label_encoder.encode_sequence(phn_list_canonical_without_sil)]\n",
    "        canonicals = torch.LongTensor(phn_encoded_list_canonical)\n",
    "        canonical_lens = torch.ones((1,1))\n",
    "\n",
    "        print(\"Getting the error stats\")\n",
    "        error_metrics = ErrorRateStats()\n",
    "        error_metrics.append(\n",
    "                        ids=[test_audio_path],\n",
    "                        predict=predicted_sequence_without_sil,\n",
    "                        target=canonicals,\n",
    "                        predict_len=None,\n",
    "                        target_len=canonical_lens,\n",
    "                        ind2lab=self.label_encoder.decode_ndim,\n",
    "                    )\n",
    "        stats = error_metrics.summarize()\n",
    "        # get score (100 - WER)\n",
    "        score = 100 - stats[\"WER\"]\n",
    "        print(f\"Calculated the score to be: {score}\")\n",
    "        print(\"Now capturing the stats sysout in a variable\")\n",
    "        # get the errors\n",
    "        # Redirect sys.stdout to capture the output\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "\n",
    "        # Call write_stats\n",
    "        error_metrics.write_stats(None)\n",
    "\n",
    "        # Get the content of the buffer\n",
    "        stats_string = sys.stdout.getvalue()\n",
    "\n",
    "        # Reset sys.stdout\n",
    "        sys.stdout = original_stdout\n",
    "        print(\"Extracting stats from stdout\")\n",
    "        return predicted_phonemes, score, self.extract_stats_from_wer_stats_string(stats_string)\n",
    "\n",
    "    def extract_stats_from_wer_stats_string(self, stats_string):\n",
    "        lines = stats_string.split('\\n')\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "        # Find the start and end of the ALIGNMENTS section\n",
    "        alignments_start = lines.index(\"ALIGNMENTS\")\n",
    "        alignments_end = lines.index(\"================================================================================\", alignments_start+1)\n",
    "        alignments_lines = lines[alignments_end+1:]\n",
    "\n",
    "        # Process alignments\n",
    "        canonical = [phn.strip() for phn in alignments_lines[1].split(';')]\n",
    "        operator = [op.strip() for op in alignments_lines[2].split(';')]\n",
    "        predicted = [phn.strip() for phn in  alignments_lines[3].split(';')]\n",
    "\n",
    "        # Initialize error categories\n",
    "        errors = {\n",
    "            \"deletions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"insertions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"substitutions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"canonical\": canonical,\n",
    "            \"predicted\": predicted\n",
    "        }\n",
    "\n",
    "        for i, item in enumerate(zip(canonical, operator, predicted)):\n",
    "            canonical_phn, op, predicted_phn = item\n",
    "            if op == \"I\":\n",
    "                errors[\"insertions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"insertions\"][\"predicted\"].append((i, predicted_phn))\n",
    "            elif op == \"S\":\n",
    "                errors[\"substitutions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"substitutions\"][\"predicted\"].append((i, predicted_phn))\n",
    "            elif op == \"D\":\n",
    "                errors[\"deletions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"deletions\"][\"predicted\"].append((i, predicted_phn))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    #Testing for one single audio file\n",
    "    def compute_forward_evaluate(self, batch, stage):\n",
    "        \"Given an input batch it computes the phoneme probabilities.\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs = batch\n",
    "\n",
    "        #creating a dummy wav_lens with shape of [batch,1] with 1\n",
    "        wav_lens = torch.ones((1,1)).to(self.device)\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        attn_mask = None\n",
    "        feats = self.modules.wav2vec2(wavs)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "        # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n",
    "        # that is, it return a list of list with different lengths\n",
    "        sequence = sb.decoders.ctc_greedy_decode(\n",
    "            p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "        )\n",
    "        transcriptions = [\" \".join(self.label_encoder.decode_ndim(s)) for s in sequence]\n",
    "        return transcriptions, sequence\n",
    "\n",
    "    def prepare_test_audio_for_inference(self, test_audio_path):\n",
    "        # Use wav2vec processor to do normalization\n",
    "        audio_signal, _ = librosa.core.load(test_audio_path, sr=self.hparams.sample_rate)\n",
    "        sig = self.hparams.wav2vec2.feature_extractor(\n",
    "            audio_signal,\n",
    "            sampling_rate=self.hparams.sample_rate,\n",
    "        ).input_values #since its only 1 file not taking [0]\n",
    "\n",
    "        sig = torch.Tensor(sig)\n",
    "        return sig\n",
    "\n",
    "    def get_predicted_phonemes_for_test_audio(self, test_audio_path):\n",
    "        print(f\"Using librosa to load the audio\")\n",
    "        batch = self.prepare_test_audio_for_inference(test_audio_path)\n",
    "        print(f\"Loading the best model & setting to eval mode\")\n",
    "        self.on_evaluate_start(min_key=\"PER\") # We call the on_evaluate_start that will load the best model\n",
    "        self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "        self.modules.wav2vec2.model.config.apply_spec_augment = False  # make sure no spec aug applied on wav2vec2\n",
    "\n",
    "        with torch.no_grad():\n",
    "          print(\"Making predictions from the best model\")\n",
    "          preds, seq = self.compute_forward_evaluate(batch, stage=sb.Stage.TEST)\n",
    "          print(\"Got the predictions\")\n",
    "        return preds, seq\n",
    "\n",
    "    def evaluate_test_audio(self, test_audio_path, canonical_phonemes):\n",
    "        predicted_phonemes, predicted_sequence = self.get_predicted_phonemes_for_test_audio(test_audio_path)\n",
    "\n",
    "        print(\"Converting canonical to appropriate format for getting error\")\n",
    "        phn_list_canonical = canonical_phonemes.strip().split()\n",
    "        phn_encoded_list_canonical = [self.label_encoder.encode_sequence(phn_list_canonical)]\n",
    "        canonicals = torch.LongTensor(phn_encoded_list_canonical)\n",
    "        canonical_lens = torch.ones((1,1))\n",
    "\n",
    "        print(\"Getting the error stats\")\n",
    "        error_metrics = ErrorRateStats()\n",
    "        error_metrics.append(\n",
    "                        ids=[test_audio_path],\n",
    "                        predict=predicted_sequence,\n",
    "                        target=canonicals,\n",
    "                        predict_len=None,\n",
    "                        target_len=canonical_lens,\n",
    "                        ind2lab=self.label_encoder.decode_ndim,\n",
    "                    )\n",
    "        stats = error_metrics.summarize()\n",
    "        # get score (100 - WER)\n",
    "        score = 100 - stats[\"WER\"]\n",
    "        print(f\"Calculated the score to be: {score}\")\n",
    "        print(\"Now capturing the stats sysout in a variable\")\n",
    "        # get the errors\n",
    "        # Redirect sys.stdout to capture the output\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "\n",
    "        # Call write_stats\n",
    "        error_metrics.write_stats(None)\n",
    "\n",
    "        # Get the content of the buffer\n",
    "        stats_string = sys.stdout.getvalue()\n",
    "\n",
    "        # Reset sys.stdout\n",
    "        sys.stdout = original_stdout\n",
    "        print(\"Extracting stats from stdout\")\n",
    "        return predicted_phonemes, score, self.extract_stats_from_wer_stats_string(stats_string)\n",
    "\n",
    "    def extract_stats_from_wer_stats_string(self, stats_string):\n",
    "        lines = stats_string.split('\\n')\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "        # Find the start and end of the ALIGNMENTS section\n",
    "        alignments_start = lines.index(\"ALIGNMENTS\")\n",
    "        alignments_end = lines.index(\"================================================================================\", alignments_start+1)\n",
    "        alignments_lines = lines[alignments_end+1:]\n",
    "\n",
    "        # Process alignments\n",
    "        canonical = [phn.strip() for phn in alignments_lines[1].split(';')]\n",
    "        operator = [op.strip() for op in alignments_lines[2].split(';')]\n",
    "        predicted = [phn.strip() for phn in  alignments_lines[3].split(';')]\n",
    "\n",
    "        # Initialize error categories\n",
    "        errors = {\n",
    "            \"deletions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"insertions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"substitutions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"canonical\": canonical,\n",
    "            \"predicted\": predicted\n",
    "        }\n",
    "\n",
    "        for i, item in enumerate(zip(canonical, operator, predicted)):\n",
    "            canonical_phn, op, predicted_phn = item\n",
    "            if op == \"I\":\n",
    "                errors[\"insertions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"insertions\"][\"predicted\"].append((i, predicted_phn))\n",
    "            elif op == \"S\":\n",
    "                errors[\"substitutions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"substitutions\"][\"predicted\"].append((i, predicted_phn))\n",
    "            elif op == \"D\":\n",
    "                errors[\"deletions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"deletions\"][\"predicted\"].append((i, predicted_phn))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    # Training code\n",
    "    def _compile_jit(self):\n",
    "        for module in self.modules:\n",
    "            if hasattr(module, \"_compile_jit\"):\n",
    "                module._compile_jit()\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"Given an input batch it computes the phoneme probabilities.\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        # phns_bos, _ = batch.phn_encoded_bos\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        # some wav2vec models (e.g. large-lv60) needs attention_mask\n",
    "        # if self.modules.wav2vec2.feature_extractor.return_attention_mask:\n",
    "        #     attn_mask = make_attn_mask(wavs, wav_lens)\n",
    "        #     feats = self.modules.wav2vec2(wavs, attention_mask=attn_mask)\n",
    "        # else:\n",
    "        #     attn_mask = None\n",
    "        #     feats = self.modules.wav2vec2(wavs)\n",
    "        feats = self.modules.wav2vec2(wavs)\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "\n",
    "        return p_ctc, wav_lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"Given the network predictions and targets computed the NLL loss.\"\n",
    "\n",
    "        p_ctc, wav_lens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        targets, target_lens = batch.phn_encoded_target\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            canonicals, canonical_lens = batch.phn_encoded_canonical\n",
    "            perceiveds, perceived_lens = batch.phn_encoded_perceived\n",
    "\n",
    "        loss_ctc = self.hparams.ctc_cost(p_ctc, targets, wav_lens, target_lens)\n",
    "        loss = loss_ctc\n",
    "\n",
    "        # Record losses for posterity\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n",
    "            # that is, it return a list of list with different lengths\n",
    "            sequence = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "            self.ctc_metrics.append(ids, p_ctc, targets, wav_lens, target_lens)\n",
    "\n",
    "            self.per_metrics.append(\n",
    "                ids=ids,\n",
    "                predict=sequence,\n",
    "                target=targets,\n",
    "                predict_len=None,\n",
    "                target_len=target_lens,\n",
    "                ind2lab=self.label_encoder.decode_ndim,\n",
    "            )\n",
    "            self.mpd_metrics.append(\n",
    "                ids=ids,\n",
    "                predict=sequence,\n",
    "                canonical=canonicals,\n",
    "                perceived=perceiveds,\n",
    "                predict_len=None,\n",
    "                canonical_len=canonical_lens,\n",
    "                perceived_len=perceived_lens,\n",
    "                ind2lab=self.label_encoder.decode_ndim,\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        predictions = self.compute_forward(batch, stage=stage)\n",
    "        loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"Gets called when a stage (either training, validation, test) starts.\"\n",
    "        self.ctc_metrics = self.hparams.ctc_stats()\n",
    "        if self.hparams.wav2vec2_specaug:\n",
    "            self.modules.wav2vec2.model.config.apply_spec_augment = True\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.modules.wav2vec2.model.config.apply_spec_augment = False\n",
    "            self.per_metrics = self.hparams.per_stats()\n",
    "            self.mpd_metrics = MpdStats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "        else:\n",
    "            per = self.per_metrics.summarize(\"error_rate\")\n",
    "            mpd_f1 = self.mpd_metrics.summarize(\"mpd_f1\")\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
    "                stage_stats[\"loss\"]\n",
    "            )\n",
    "            old_lr_wav2vec2, new_lr_wav2vec2 = self.hparams.lr_annealing_wav2vec2(\n",
    "                stage_stats[\"loss\"]\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.adam_optimizer, new_lr_model\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.wav2vec_optimizer, new_lr_wav2vec2\n",
    "            )\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"lr_model\": old_lr_model,\n",
    "                    \"lr_wav2vec2\": old_lr_wav2vec2,\n",
    "                },\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats={\n",
    "                    \"loss\": stage_loss,\n",
    "                    \"ctc_loss\": self.ctc_metrics.summarize(\"average\"),\n",
    "                    \"PER\": per,\n",
    "                    \"mpd_f1\": mpd_f1\n",
    "                },\n",
    "            )\n",
    "\n",
    "\n",
    "            # self.hparams.train_logger.log_stats(\n",
    "            #     stats_meta={\n",
    "            #         \"epoch\": epoch,\n",
    "            #         \"lr_adam\": self.adam_optimizer.param_groups[0][\"lr\"],\n",
    "            #         \"lr_wav2vec\": self.wav2vec_optimizer.param_groups[0][\"lr\"],\n",
    "            #     },\n",
    "            #     train_stats={\"loss\": self.train_loss},\n",
    "            #     valid_stats={\n",
    "            #         \"loss\": stage_loss,\n",
    "            #         \"ctc_loss\": self.ctc_metrics.summarize(\"average\"),\n",
    "            #         \"PER\": per,\n",
    "            #         \"mpd_f1\": mpd_f1\n",
    "            #     },\n",
    "            # )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"PER\": per, \"mpd_f1\": mpd_f1}, min_keys=[\"PER\"], max_keys=[\"mpd_f1\"]\n",
    "            )\n",
    "\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats={\"loss\": stage_loss, \"PER\": per, \"mpd_f1\": mpd_f1},\n",
    "            )\n",
    "            with open(self.hparams.wer_file, \"w\") as w:\n",
    "                w.write(\"CTC loss stats:\\n\")\n",
    "                self.ctc_metrics.write_stats(w)\n",
    "                w.write(\"\\nPER stats:\\n\")\n",
    "                self.per_metrics.write_stats(w)\n",
    "                print(\n",
    "                    \"CTC and PER stats written to file\",\n",
    "                    self.hparams.wer_file,\n",
    "                )\n",
    "            with open(self.hparams.mpd_file, \"w\") as m:\n",
    "                m.write(\"MPD results and stats:\\n\")\n",
    "                self.mpd_metrics.write_stats(m)\n",
    "                print(\n",
    "                    \"MPD results and stats written to file\",\n",
    "                    self.hparams.mpd_file,\n",
    "                )\n",
    "\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Fit one batch, override to do multiple updates.\n",
    "\n",
    "        The default implementation depends on a few methods being defined\n",
    "        with a particular behavior:\n",
    "\n",
    "        * ``compute_forward()``\n",
    "        * ``compute_objectives()``\n",
    "\n",
    "        Also depends on having optimizers passed at initialization.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : list of torch.Tensors\n",
    "            Batch of data to use for training. Default implementation assumes\n",
    "            this batch has two elements: inputs and targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        detached loss\n",
    "        \"\"\"\n",
    "        # Managing automatic mixed precision\n",
    "        if self.auto_mix_prec:\n",
    "\n",
    "            self.wav2vec_optimizer.zero_grad()\n",
    "            self.adam_optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.wav2vec_optimizer)\n",
    "            self.scaler.unscale_(self.adam_optimizer)\n",
    "\n",
    "            if self.check_gradients(loss):\n",
    "                self.scaler.step(self.wav2vec_optimizer)\n",
    "                self.scaler.step(self.adam_optimizer)\n",
    "\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "\n",
    "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            # normalize the loss by gradient_accumulation step\n",
    "            (loss / self.hparams.gradient_accumulation).backward()\n",
    "\n",
    "            if self.step % self.hparams.gradient_accumulation == 0:\n",
    "                # gradient clipping & early stop if loss is not fini\n",
    "                if self.check_gradients(loss):\n",
    "                    self.wav2vec_optimizer.step()\n",
    "                    self.adam_optimizer.step()\n",
    "\n",
    "                self.wav2vec_optimizer.zero_grad()\n",
    "                self.adam_optimizer.zero_grad()\n",
    "\n",
    "        return loss.detach().cpu()\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
    "        self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "            self.modules.wav2vec2.model.parameters()\n",
    "        )\n",
    "        self.adam_optimizer = self.hparams.adam_opt_class(\n",
    "            self.hparams.model.parameters()\n",
    "        )\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec_opt\", self.wav2vec_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"adam_opt\", self.adam_optimizer)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Gets called at the beginning of ``fit()``, on multiple processes\n",
    "        if ``distributed_count > 0`` and backend is ddp.\n",
    "\n",
    "        Default implementation compiles the jit modules, initializes\n",
    "        optimizers, and loads the latest checkpoint to resume training.\n",
    "        \"\"\"\n",
    "        # Run this *after* starting all processes since jit modules cannot be\n",
    "        # pickled.\n",
    "        self._compile_jit()\n",
    "\n",
    "        # Wrap modules with parallel backend after jit\n",
    "        self._wrap_distributed()\n",
    "\n",
    "        # Initialize optimizers after parameters are configured\n",
    "        self.init_optimizers()\n",
    "\n",
    "        # Load latest checkpoint to resume training if interrupted\n",
    "        ## NOTE: make sure to use the \"best\" model to continual training\n",
    "        ## so we set the `min_key` argument\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.recover_if_possible(\n",
    "                device=torch.device(self.device),\n",
    "                min_key=\"PER\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Brain Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfd0QdzCeDuX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726544259,
     "user_tz": -480,
     "elapsed": 397,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    }
   },
   "outputs": [],
   "source": [
    "def dataio_prep(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = hparams[\"data_folder_save\"]\n",
    "    # 1. Declarations:\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        # we sort training data to speed up training and get better results.\n",
    "        train_data = train_data.filtered_sorted(sort_key=\"duration\")\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"test_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    test_data = test_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    datasets = [train_data, valid_data, test_data]\n",
    "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
    "\n",
    "    # 2. Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        # sig = sb.dataio.dataio.read_audio(wav)\n",
    "        # # sample rate change to 16000, e,g, using librosa\n",
    "        # sig = torch.Tensor(librosa.core.load(wav, hparams[\"sample_rate\"])[0])\n",
    "        # Use wav2vec processor to do normalization\n",
    "        sig = hparams[\"wav2vec2\"].feature_extractor(\n",
    "            librosa.core.load(wav, sr=hparams[\"sample_rate\"])[0],\n",
    "            sampling_rate=hparams[\"sample_rate\"],\n",
    "        ).input_values[0]\n",
    "        sig = torch.Tensor(sig)\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "    # 3. Define text pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"perceived_train_target\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"phn_list_target\",\n",
    "        \"phn_encoded_list_target\",\n",
    "        \"phn_encoded_target\",\n",
    "    )\n",
    "    def text_pipeline_train(phn):\n",
    "        phn_list = phn.strip().split()\n",
    "        yield phn_list\n",
    "        phn_encoded_list = label_encoder.encode_sequence(phn_list)\n",
    "        yield phn_encoded_list\n",
    "        phn_encoded = torch.LongTensor(phn_encoded_list)\n",
    "        yield phn_encoded\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"perceived_train_target\", \"canonical_aligned\", \"perceived_aligned\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"phn_list_target\",\n",
    "        \"phn_encoded_list_target\",\n",
    "        \"phn_encoded_target\",\n",
    "        \"phn_list_canonical\",\n",
    "        \"phn_encoded_list_canonical\",\n",
    "        \"phn_encoded_canonical\",\n",
    "        \"phn_list_perceived\",\n",
    "        \"phn_encoded_list_perceived\",\n",
    "        \"phn_encoded_perceived\",\n",
    "    )\n",
    "    def text_pipeline_test(target, canonical, perceived):\n",
    "        phn_list_target = target.strip().split()\n",
    "        yield phn_list_target\n",
    "        phn_encoded_list_target = label_encoder.encode_sequence(phn_list_target)\n",
    "        yield phn_encoded_list_target\n",
    "        phn_encoded_target = torch.LongTensor(phn_encoded_list_target)\n",
    "        yield phn_encoded_target\n",
    "        phn_list_canonical = canonical.strip().split()\n",
    "        yield phn_list_canonical\n",
    "        phn_encoded_list_canonical = label_encoder.encode_sequence(phn_list_canonical)\n",
    "        yield phn_encoded_list_canonical\n",
    "        phn_encoded_canonical = torch.LongTensor(phn_encoded_list_canonical)\n",
    "        yield phn_encoded_canonical\n",
    "        phn_list_perceived = perceived.strip().split()\n",
    "        yield phn_list_perceived\n",
    "        phn_encoded_list_perceived = label_encoder.encode_sequence(phn_list_perceived)\n",
    "        yield phn_encoded_list_perceived\n",
    "        phn_encoded_perceived = torch.LongTensor(phn_encoded_list_perceived)\n",
    "        yield phn_encoded_perceived\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item([train_data], text_pipeline_train)\n",
    "    sb.dataio.dataset.add_dynamic_item([valid_data, test_data], text_pipeline_test)\n",
    "\n",
    "    # 3. Fit encoder:\n",
    "    # Load or compute the label encoder\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    special_labels = {\n",
    "        \"blank_label\": hparams[\"blank_index\"],\n",
    "    }\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[train_data],\n",
    "        output_key=\"phn_list_target\",\n",
    "        special_labels=special_labels,\n",
    "        sequence_input=True,\n",
    "    )\n",
    "\n",
    "    # 4. Set output:\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        [train_data],\n",
    "        [\"id\", \"sig\", \"phn_encoded_target\"],\n",
    "    )\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        [valid_data, test_data],\n",
    "        [\"id\", \"sig\", \"phn_encoded_target\", \"phn_encoded_canonical\", \"phn_encoded_perceived\"],\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_data, label_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating data loaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVsbqzMyJBRL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700726623577,
     "user_tz": -480,
     "elapsed": 46188,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "referenced_widgets": [
      "f1e92059b9d640fe9f734a2bca0f61ca",
      "274357a698c44d84a0dd28ccdfceba93",
      "89cdc42e69804e27b3ebf5bab8381a16",
      "d17b20c05d9c4ed18fc6d92e08cef520",
      "3ccb055e457142e69d8f39d772be7f8a",
      "02c894167725425ab2700ece21d0e0bd",
      "d8bee966d11a4f60a58ae5694857117e",
      "9095d177164243d6855b326d3098a57e",
      "332639be72f545c0a912103f4b01c6c5",
      "6829239555414ecc822c9ad3f6773832",
      "4ed391baf64342458997b91458af15a0",
      "8db26b016cd44f75aff900b825ca5698",
      "00ee73e2d111400e9c7c610797830630",
      "cff447359bfc4b92b06cb14199ac922f",
      "f7026a1e76534aab9f77a9b031ff5d83",
      "659c6aaf1e8f47aeac36a76e318ed268",
      "0dd6f10739a141fdaebb66d440c6e00c",
      "8e0b650828704a9383fa9fa229d66bc8",
      "9d3a1b95b9d24919989a94bf92476b54",
      "d60e6d8e578346e19b5304a9755acc66",
      "95a44fc7fd9143f4a7581edf95024af6",
      "bb056833647b454da897baba328557c3",
      "9244136038b241b7a9822899100b40dc",
      "135c5b9ef5954274ae2514d42e06192b",
      "9db1f123870844af9d34c923721ba66d",
      "8e3b1ec4b7c248509ea36832180f5652",
      "92bdb02fbab8457fb64873e67f9c556d",
      "95e4874b1f954a39940ed62e5dfe9b2d",
      "2d768e45e8cc4ccbb14fc8f61052ac49",
      "9749adf2eeef4355bee3cd69b0e4fc57",
      "ebc91180aa4c4f13b456a55f02c0289e",
      "07e02cafbef84c91889297643dd93c87",
      "eb1c7d32488c441dae5a42040e5c4006"
     ]
    },
    "outputId": "7de64a4c-7dec-4038-fe2f-625e3bd413ae"
   },
   "outputs": [],
   "source": [
    "\n",
    "hparams_file = '/content/drive/MyDrive/CS5647_Project/hparams/hubert_train.yaml'\n",
    "\n",
    "# Load hyperparameters file with command-line overrides\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)\n",
    "\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    ")\n",
    "\n",
    "# Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
    "train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initializing the ASR Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"12\""
   ],
   "metadata": {
    "id": "-b_FGnT7CaUe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700727186724,
     "user_tz": -480,
     "elapsed": 719,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQHyc36_74th",
    "executionInfo": {
     "status": "error",
     "timestamp": 1700727540884,
     "user_tz": -480,
     "elapsed": 353474,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "outputId": "d3a2fc1f-f6a6-4f8e-aba6-c5e4720b6a27"
   },
   "outputs": [],
   "source": [
    "# Trainer initialization\n",
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    "    run_opts = {\"device\": \"cuda\"}\n",
    ")\n",
    "asr_brain.label_encoder = label_encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the ASR Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training/validation loop\n",
    "asr_brain.fit(\n",
    "    asr_brain.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the ASR Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test\n",
    "asr_brain.evaluate(\n",
    "    test_data,\n",
    "    min_key=\"PER\",\n",
    "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4tSMlfL7lCb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700664556614,
     "user_tz": -480,
     "elapsed": 141428,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    },
    "outputId": "9dbae97e-3366-4f5c-a98e-da95809836e6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making inference on a single audio file"
   ],
   "metadata": {
    "id": "Njo4uu4Zl8cJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_audio_path = \"/content/drive/MyDrive/CS5647_Project/dataset/TNI/wav/arctic_a0100.wav\"\n",
    "canonical_phonemes = \"sil y uw m ah s t s l iy p sil hh iy er jh d sil sil\" # actual sentence is 'You must sleep he urged'\n",
    "predicted_phonemes, score, stats = asr_brain.evaluate_test_audio(test_audio_path, canonical_phonemes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edd0Y_StHhIw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700662774633,
     "user_tz": -480,
     "elapsed": 1072995,
     "user": {
      "displayName": "lalitha ravi",
      "userId": "00357164084518936800"
     }
    },
    "outputId": "638f3c5b-858c-45ad-b911-34af46f18c52"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "predicted_phonemes"
   ],
   "metadata": {
    "id": "nh60DUuPViI5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "f1e92059b9d640fe9f734a2bca0f61ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_274357a698c44d84a0dd28ccdfceba93",
       "IPY_MODEL_89cdc42e69804e27b3ebf5bab8381a16",
       "IPY_MODEL_d17b20c05d9c4ed18fc6d92e08cef520"
      ],
      "layout": "IPY_MODEL_3ccb055e457142e69d8f39d772be7f8a"
     }
    },
    "274357a698c44d84a0dd28ccdfceba93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02c894167725425ab2700ece21d0e0bd",
      "placeholder": "​",
      "style": "IPY_MODEL_d8bee966d11a4f60a58ae5694857117e",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "89cdc42e69804e27b3ebf5bab8381a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9095d177164243d6855b326d3098a57e",
      "max": 212,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_332639be72f545c0a912103f4b01c6c5",
      "value": 212
     }
    },
    "d17b20c05d9c4ed18fc6d92e08cef520": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6829239555414ecc822c9ad3f6773832",
      "placeholder": "​",
      "style": "IPY_MODEL_4ed391baf64342458997b91458af15a0",
      "value": " 212/212 [00:00&lt;00:00, 17.6kB/s]"
     }
    },
    "3ccb055e457142e69d8f39d772be7f8a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02c894167725425ab2700ece21d0e0bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8bee966d11a4f60a58ae5694857117e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9095d177164243d6855b326d3098a57e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "332639be72f545c0a912103f4b01c6c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6829239555414ecc822c9ad3f6773832": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ed391baf64342458997b91458af15a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8db26b016cd44f75aff900b825ca5698": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_00ee73e2d111400e9c7c610797830630",
       "IPY_MODEL_cff447359bfc4b92b06cb14199ac922f",
       "IPY_MODEL_f7026a1e76534aab9f77a9b031ff5d83"
      ],
      "layout": "IPY_MODEL_659c6aaf1e8f47aeac36a76e318ed268"
     }
    },
    "00ee73e2d111400e9c7c610797830630": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dd6f10739a141fdaebb66d440c6e00c",
      "placeholder": "​",
      "style": "IPY_MODEL_8e0b650828704a9383fa9fa229d66bc8",
      "value": "config.json: 100%"
     }
    },
    "cff447359bfc4b92b06cb14199ac922f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d3a1b95b9d24919989a94bf92476b54",
      "max": 1376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d60e6d8e578346e19b5304a9755acc66",
      "value": 1376
     }
    },
    "f7026a1e76534aab9f77a9b031ff5d83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95a44fc7fd9143f4a7581edf95024af6",
      "placeholder": "​",
      "style": "IPY_MODEL_bb056833647b454da897baba328557c3",
      "value": " 1.38k/1.38k [00:00&lt;00:00, 98.6kB/s]"
     }
    },
    "659c6aaf1e8f47aeac36a76e318ed268": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dd6f10739a141fdaebb66d440c6e00c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e0b650828704a9383fa9fa229d66bc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d3a1b95b9d24919989a94bf92476b54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d60e6d8e578346e19b5304a9755acc66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95a44fc7fd9143f4a7581edf95024af6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb056833647b454da897baba328557c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9244136038b241b7a9822899100b40dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_135c5b9ef5954274ae2514d42e06192b",
       "IPY_MODEL_9db1f123870844af9d34c923721ba66d",
       "IPY_MODEL_8e3b1ec4b7c248509ea36832180f5652"
      ],
      "layout": "IPY_MODEL_92bdb02fbab8457fb64873e67f9c556d"
     }
    },
    "135c5b9ef5954274ae2514d42e06192b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95e4874b1f954a39940ed62e5dfe9b2d",
      "placeholder": "​",
      "style": "IPY_MODEL_2d768e45e8cc4ccbb14fc8f61052ac49",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "9db1f123870844af9d34c923721ba66d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9749adf2eeef4355bee3cd69b0e4fc57",
      "max": 1262057559,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ebc91180aa4c4f13b456a55f02c0289e",
      "value": 1262057559
     }
    },
    "8e3b1ec4b7c248509ea36832180f5652": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07e02cafbef84c91889297643dd93c87",
      "placeholder": "​",
      "style": "IPY_MODEL_eb1c7d32488c441dae5a42040e5c4006",
      "value": " 1.26G/1.26G [00:36&lt;00:00, 35.4MB/s]"
     }
    },
    "92bdb02fbab8457fb64873e67f9c556d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95e4874b1f954a39940ed62e5dfe9b2d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d768e45e8cc4ccbb14fc8f61052ac49": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9749adf2eeef4355bee3cd69b0e4fc57": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebc91180aa4c4f13b456a55f02c0289e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "07e02cafbef84c91889297643dd93c87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb1c7d32488c441dae5a42040e5c4006": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
