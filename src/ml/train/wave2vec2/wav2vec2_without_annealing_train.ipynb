{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing required packages"
   ],
   "metadata": {
    "id": "sWUkD-DTGaD7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWkjs_8PHcBf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b6655b96-f918-4d14-f9e3-e1719b71b06d"
   },
   "outputs": [],
   "source": [
    "!pip install speechbrain\n",
    "!pip install textgrid transformers librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing requried modules"
   ],
   "metadata": {
    "id": "Xn1PV5KnGeN2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53nh8Xl--eZ4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import librosa\n",
    "import csv\n",
    "from google.colab import drive, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mounting google drive"
   ],
   "metadata": {
    "id": "ZCERYWlXGkTd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiA9SjWIHOx5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5e161af6-7c1d-4691-d90b-6b3af4848f9b"
   },
   "outputs": [],
   "source": [
    "# Mount drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Constants & Importing Eval metric file"
   ],
   "metadata": {
    "id": "tBhwzhbEG2gZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sikO-MV-Iv0V",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5e21f4fa-418e-41e4-d25f-2add68a4170c"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "folder_path = '/content/drive/MyDrive/CS5647_Project'\n",
    "os.chdir(folder_path)\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory after change:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1KP_dzhN-A7"
   },
   "outputs": [],
   "source": [
    "from mpd_eval_v3 import MpdStats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Speech Brain object"
   ],
   "metadata": {
    "id": "QY3NauW8G-jU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3meQIqYI1tr"
   },
   "outputs": [],
   "source": [
    "def make_attn_mask(wavs, wav_lens):\n",
    "    \"\"\"\n",
    "    wav_lens: relative lengths(i.e. 0-1) of a batch. shape: (bs, )\n",
    "    return a tensor of shape (bs, seq_len), representing mask on allowed positions.\n",
    "            1 for regular tokens, 0 for padded tokens\n",
    "    \"\"\"\n",
    "    abs_lens = (wav_lens*wavs.shape[1]).long()\n",
    "    attn_mask = wavs.new(wavs.shape).zero_().long()\n",
    "    for i in range(len(abs_lens)):\n",
    "        attn_mask[i, :abs_lens[i]] = 1\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRjl6kGqI7Lr"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from speechbrain.utils.metric_stats import ErrorRateStats\n",
    "\n",
    "class ASR(sb.Brain):\n",
    "    #Testing for one single audio file\n",
    "    def compute_forward_evaluate(self, batch, stage):\n",
    "        \"Given an input batch it computes the phoneme probabilities.\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs = batch\n",
    "\n",
    "        #creating a dummy wav_lens with shape of [batch,1] with 1\n",
    "        wav_lens = torch.ones((1,1)).to(self.device)\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        attn_mask = None\n",
    "        feats = self.modules.wav2vec2(wavs)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "        # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n",
    "        # that is, it return a list of list with different lengths\n",
    "        sequence = sb.decoders.ctc_greedy_decode(\n",
    "            p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "        )\n",
    "        transcriptions = [\" \".join(self.label_encoder.decode_ndim(s)) for s in sequence]\n",
    "        return transcriptions, sequence\n",
    "\n",
    "    def prepare_test_audio_for_inference(self, test_audio_path):\n",
    "        # Use wav2vec processor to do normalization\n",
    "        audio_signal, _ = librosa.core.load(test_audio_path, sr=self.hparams.sample_rate)\n",
    "        sig = self.hparams.wav2vec2.feature_extractor(\n",
    "            audio_signal,\n",
    "            sampling_rate=self.hparams.sample_rate,\n",
    "        ).input_values #since its only 1 file not taking [0]\n",
    "\n",
    "        sig = torch.Tensor(sig)\n",
    "        return sig\n",
    "\n",
    "    def get_predicted_phonemes_for_test_audio(self, test_audio_path):\n",
    "        print(f\"Using librosa to load the audio\")\n",
    "        batch = self.prepare_test_audio_for_inference(test_audio_path)\n",
    "        print(f\"Loading the best model & setting to eval mode\")\n",
    "        self.on_evaluate_start(min_key=\"PER\") # We call the on_evaluate_start that will load the best model\n",
    "        self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "        self.modules.wav2vec2.model.config.apply_spec_augment = False  # make sure no spec aug applied on wav2vec2\n",
    "\n",
    "        with torch.no_grad():\n",
    "          print(\"Making predictions from the best model\")\n",
    "          preds, seq = self.compute_forward_evaluate(batch, stage=sb.Stage.TEST)\n",
    "          print(f\"Got the predictions which is {preds}\")\n",
    "        return preds[0], seq\n",
    "\n",
    "    def evaluate_test_audio(self, test_audio_path, canonical_phonemes):\n",
    "        predicted_phonemes, predicted_sequence = self.get_predicted_phonemes_for_test_audio(test_audio_path)\n",
    "        predicted_phonemes = predicted_phonemes.split()\n",
    "        predicted_sequence_without_sil = [[]]\n",
    "\n",
    "        # Ensure that we remove the sils\n",
    "        for pred_phoneme, pred_seq in zip(predicted_phonemes, predicted_sequence[0]):\n",
    "            if pred_phoneme != \"sil\":\n",
    "                predicted_sequence_without_sil[0].append(pred_seq)\n",
    "\n",
    "\n",
    "        print(\"Converting canonical to appropriate format for getting error\")\n",
    "        phn_list_canonical = canonical_phonemes.strip().split()\n",
    "        phn_list_canonical_without_sil = list(filter(lambda phn: phn != \"sil\", phn_list_canonical))\n",
    "        phn_encoded_list_canonical = [self.label_encoder.encode_sequence(phn_list_canonical_without_sil)]\n",
    "        canonicals = torch.LongTensor(phn_encoded_list_canonical)\n",
    "        canonical_lens = torch.ones((1,1))\n",
    "\n",
    "        print(\"Getting the error stats\")\n",
    "        error_metrics = ErrorRateStats()\n",
    "        error_metrics.append(\n",
    "                        ids=[test_audio_path],\n",
    "                        predict=predicted_sequence_without_sil,\n",
    "                        target=canonicals,\n",
    "                        predict_len=None,\n",
    "                        target_len=canonical_lens,\n",
    "                        ind2lab=self.label_encoder.decode_ndim,\n",
    "                    )\n",
    "        stats = error_metrics.summarize()\n",
    "        # get score (100 - WER)\n",
    "        score = 100 - stats[\"WER\"]\n",
    "        print(f\"Calculated the score to be: {score}\")\n",
    "        print(\"Now capturing the stats sysout in a variable\")\n",
    "        # get the errors\n",
    "        # Redirect sys.stdout to capture the output\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "\n",
    "        # Call write_stats\n",
    "        error_metrics.write_stats(None)\n",
    "\n",
    "        # Get the content of the buffer\n",
    "        stats_string = sys.stdout.getvalue()\n",
    "\n",
    "        # Reset sys.stdout\n",
    "        sys.stdout = original_stdout\n",
    "        print(\"Extracting stats from stdout\")\n",
    "        return predicted_sequence_without_sil, score, self.extract_stats_from_wer_stats_string(stats_string)\n",
    "\n",
    "    def extract_stats_from_wer_stats_string(self, stats_string):\n",
    "        lines = stats_string.split('\\n')\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "        # Find the start and end of the ALIGNMENTS section\n",
    "        alignments_start = lines.index(\"ALIGNMENTS\")\n",
    "        alignments_end = lines.index(\"================================================================================\", alignments_start+1)\n",
    "        alignments_lines = lines[alignments_end+1:]\n",
    "\n",
    "        # Process alignments\n",
    "        canonical = [phn.strip() for phn in alignments_lines[1].split(';')]\n",
    "        operator = [op.strip() for op in alignments_lines[2].split(';')]\n",
    "        predicted = [phn.strip() for phn in  alignments_lines[3].split(';')]\n",
    "\n",
    "        # Initialize error categories\n",
    "        errors = {\n",
    "            \"deletions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"insertions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"substitutions\": {\"canonical\": [], \"predicted\": []},\n",
    "            \"canonical\": canonical,\n",
    "            \"predicted\": predicted\n",
    "        }\n",
    "\n",
    "        for i, item in enumerate(zip(canonical, operator, predicted)):\n",
    "            canonical_phn, op, predicted_phn = item\n",
    "            if op == \"I\":\n",
    "                errors[\"insertions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"insertions\"][\"predicted\"].append((i, predicted_phn))\n",
    "            elif op == \"S\":\n",
    "                errors[\"substitutions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"substitutions\"][\"predicted\"].append((i, predicted_phn))\n",
    "            elif op == \"D\":\n",
    "                errors[\"deletions\"][\"canonical\"].append((i, canonical_phn))\n",
    "                errors[\"deletions\"][\"predicted\"].append((i, predicted_phn))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    #Training\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"Given an input batch it computes the phoneme probabilities.\"\n",
    "        batch = batch.to(self.device)\n",
    "\n",
    "        wavs, wav_lens = batch.sig\n",
    "        # phns_bos, _ = batch.phn_encoded_bos\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        # some wav2vec models (e.g. large-lv60) needs attention_mask\n",
    "        if self.modules.wav2vec2.feature_extractor.return_attention_mask:\n",
    "            attn_mask = make_attn_mask(wavs, wav_lens)\n",
    "            feats = self.modules.wav2vec2(wavs, attention_mask=attn_mask)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "            feats = self.modules.wav2vec2(wavs)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "\n",
    "        return p_ctc, wav_lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"Given the network predictions and targets computed the NLL loss.\"\n",
    "\n",
    "        p_ctc, wav_lens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        targets, target_lens = batch.phn_encoded_target\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            canonicals, canonical_lens = batch.phn_encoded_canonical\n",
    "            perceiveds, perceived_lens = batch.phn_encoded_perceived\n",
    "\n",
    "        loss_ctc = self.hparams.ctc_cost(p_ctc, targets, wav_lens, target_lens)\n",
    "        loss = loss_ctc\n",
    "\n",
    "        # Record losses for posterity\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n",
    "            # that is, it return a list of list with different lengths\n",
    "            sequence = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "            self.ctc_metrics.append(ids, p_ctc, targets, wav_lens, target_lens)\n",
    "\n",
    "            self.per_metrics.append(\n",
    "                ids=ids,\n",
    "                predict=sequence,\n",
    "                target=targets,\n",
    "                predict_len=None,\n",
    "                target_len=target_lens,\n",
    "                ind2lab=self.label_encoder.decode_ndim,\n",
    "            )\n",
    "            self.mpd_metrics.append(\n",
    "                ids=ids,\n",
    "                predict=sequence,\n",
    "                canonical=canonicals,\n",
    "                perceived=perceiveds,\n",
    "                predict_len=None,\n",
    "                canonical_len=canonical_lens,\n",
    "                perceived_len=perceived_lens,\n",
    "                ind2lab=self.label_encoder.decode_ndim,\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        predictions = self.compute_forward(batch, stage=stage)\n",
    "        loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"Gets called when a stage (either training, validation, test) starts.\"\n",
    "        self.ctc_metrics = self.hparams.ctc_stats()\n",
    "        if self.hparams.wav2vec2_specaug:\n",
    "            self.modules.wav2vec2.model.config.apply_spec_augment = True\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.modules.wav2vec2.model.config.apply_spec_augment = False\n",
    "            self.per_metrics = self.hparams.per_stats()\n",
    "            self.mpd_metrics = MpdStats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "        else:\n",
    "            per = self.per_metrics.summarize(\"error_rate\")\n",
    "            mpd_f1 = self.mpd_metrics.summarize(\"mpd_f1\")\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
    "                stage_loss\n",
    "            )\n",
    "            old_lr_wav2vec2, new_lr_wav2vec2 = self.hparams.lr_annealing_wav2vec2(\n",
    "                stage_loss\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.adam_optimizer, new_lr_model\n",
    "            )\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.wav2vec_optimizer, new_lr_wav2vec2\n",
    "            )\n",
    "\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"lr_adam\": self.adam_optimizer.param_groups[0][\"lr\"],\n",
    "                    \"lr_wav2vec\": self.wav2vec_optimizer.param_groups[0][\"lr\"],\n",
    "                },\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats={\n",
    "                    \"loss\": stage_loss,\n",
    "                    \"ctc_loss\": self.ctc_metrics.summarize(\"average\"),\n",
    "                    \"PER\": per,\n",
    "                    \"mpd_f1\": mpd_f1\n",
    "                },\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"PER\": per, \"mpd_f1\": mpd_f1}, min_keys=[\"PER\"], max_keys=[\"mpd_f1\"]\n",
    "            )\n",
    "\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats={\"loss\": stage_loss, \"PER\": per, \"mpd_f1\": mpd_f1},\n",
    "            )\n",
    "            with open(self.hparams.wer_file, \"w\") as w:\n",
    "                w.write(\"CTC loss stats:\\n\")\n",
    "                self.ctc_metrics.write_stats(w)\n",
    "                w.write(\"\\nPER stats:\\n\")\n",
    "                self.per_metrics.write_stats(w)\n",
    "                print(\n",
    "                    \"CTC and PER stats written to file\",\n",
    "                    self.hparams.wer_file,\n",
    "                )\n",
    "            with open(self.hparams.mpd_file, \"w\") as m:\n",
    "                m.write(\"MPD results and stats:\\n\")\n",
    "                self.mpd_metrics.write_stats(m)\n",
    "                print(\n",
    "                    \"MPD results and stats written to file\",\n",
    "                    self.hparams.mpd_file,\n",
    "                )\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Fit one batch, override to do multiple updates.\n",
    "\n",
    "        The default implementation depends on a few methods being defined\n",
    "        with a particular behavior:\n",
    "\n",
    "        * ``compute_forward()``\n",
    "        * ``compute_objectives()``\n",
    "\n",
    "        Also depends on having optimizers passed at initialization.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : list of torch.Tensors\n",
    "            Batch of data to use for training. Default implementation assumes\n",
    "            this batch has two elements: inputs and targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        detached loss\n",
    "        \"\"\"\n",
    "        # Managing automatic mixed precision\n",
    "        if self.auto_mix_prec:\n",
    "\n",
    "            self.wav2vec_optimizer.zero_grad()\n",
    "            self.adam_optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.wav2vec_optimizer)\n",
    "            self.scaler.unscale_(self.adam_optimizer)\n",
    "\n",
    "            if self.check_gradients(loss):\n",
    "                self.scaler.step(self.wav2vec_optimizer)\n",
    "                self.scaler.step(self.adam_optimizer)\n",
    "\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "\n",
    "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            # normalize the loss by gradient_accumulation step\n",
    "            (loss / self.hparams.gradient_accumulation).backward()\n",
    "\n",
    "            if self.step % self.hparams.gradient_accumulation == 0:\n",
    "                # gradient clipping & early stop if loss is not fini\n",
    "                if self.check_gradients(loss):\n",
    "                    self.wav2vec_optimizer.step()\n",
    "                    self.adam_optimizer.step()\n",
    "\n",
    "                self.wav2vec_optimizer.zero_grad()\n",
    "                self.adam_optimizer.zero_grad()\n",
    "\n",
    "        return loss.detach().cpu()\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
    "        self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "            self.modules.wav2vec2.model.parameters()\n",
    "        )\n",
    "        self.adam_optimizer = self.hparams.adam_opt_class(\n",
    "            self.hparams.model.parameters()\n",
    "        )\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec_opt\", self.wav2vec_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"adam_opt\", self.adam_optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Brain Data preparation"
   ],
   "metadata": {
    "id": "dkrT2roeHHwZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfd0QdzCeDuX"
   },
   "outputs": [],
   "source": [
    "def dataio_prep(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = hparams[\"data_folder_save\"]\n",
    "    # 1. Declarations:\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        # we sort training data to speed up training and get better results.\n",
    "        train_data = train_data.filtered_sorted(sort_key=\"duration\")\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"test_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    test_data = test_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    datasets = [train_data, valid_data, test_data]\n",
    "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
    "\n",
    "    # 2. Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        # sig = sb.dataio.dataio.read_audio(wav)\n",
    "        # # sample rate change to 16000, e,g, using librosa\n",
    "        # sig = torch.Tensor(librosa.core.load(wav, hparams[\"sample_rate\"])[0])\n",
    "        # Use wav2vec processor to do normalization\n",
    "        sig = hparams[\"wav2vec2\"].feature_extractor(\n",
    "            librosa.core.load(wav, sr=hparams[\"sample_rate\"])[0],\n",
    "            sampling_rate=hparams[\"sample_rate\"],\n",
    "        ).input_values[0]\n",
    "        sig = torch.Tensor(sig)\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "    # 3. Define text pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"perceived_train_target\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"phn_list_target\",\n",
    "        \"phn_encoded_list_target\",\n",
    "        \"phn_encoded_target\",\n",
    "    )\n",
    "    def text_pipeline_train(phn):\n",
    "        phn_list = phn.strip().split()\n",
    "        yield phn_list\n",
    "        phn_encoded_list = label_encoder.encode_sequence(phn_list)\n",
    "        yield phn_encoded_list\n",
    "        phn_encoded = torch.LongTensor(phn_encoded_list)\n",
    "        yield phn_encoded\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"perceived_train_target\", \"canonical_aligned\", \"perceived_aligned\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"phn_list_target\",\n",
    "        \"phn_encoded_list_target\",\n",
    "        \"phn_encoded_target\",\n",
    "        \"phn_list_canonical\",\n",
    "        \"phn_encoded_list_canonical\",\n",
    "        \"phn_encoded_canonical\",\n",
    "        \"phn_list_perceived\",\n",
    "        \"phn_encoded_list_perceived\",\n",
    "        \"phn_encoded_perceived\",\n",
    "    )\n",
    "    def text_pipeline_test(target, canonical, perceived):\n",
    "        phn_list_target = target.strip().split()\n",
    "        yield phn_list_target\n",
    "        phn_encoded_list_target = label_encoder.encode_sequence(phn_list_target)\n",
    "        yield phn_encoded_list_target\n",
    "        phn_encoded_target = torch.LongTensor(phn_encoded_list_target)\n",
    "        yield phn_encoded_target\n",
    "        phn_list_canonical = canonical.strip().split()\n",
    "        yield phn_list_canonical\n",
    "        phn_encoded_list_canonical = label_encoder.encode_sequence(phn_list_canonical)\n",
    "        yield phn_encoded_list_canonical\n",
    "        phn_encoded_canonical = torch.LongTensor(phn_encoded_list_canonical)\n",
    "        yield phn_encoded_canonical\n",
    "        phn_list_perceived = perceived.strip().split()\n",
    "        yield phn_list_perceived\n",
    "        phn_encoded_list_perceived = label_encoder.encode_sequence(phn_list_perceived)\n",
    "        yield phn_encoded_list_perceived\n",
    "        phn_encoded_perceived = torch.LongTensor(phn_encoded_list_perceived)\n",
    "        yield phn_encoded_perceived\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item([train_data], text_pipeline_train)\n",
    "    sb.dataio.dataset.add_dynamic_item([valid_data, test_data], text_pipeline_test)\n",
    "\n",
    "    # 3. Fit encoder:\n",
    "    # Load or compute the label encoder\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    special_labels = {\n",
    "        \"blank_label\": hparams[\"blank_index\"],\n",
    "    }\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[train_data],\n",
    "        output_key=\"phn_list_target\",\n",
    "        special_labels=special_labels,\n",
    "        sequence_input=True,\n",
    "    )\n",
    "\n",
    "    # 4. Set output:\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        [train_data],\n",
    "        [\"id\", \"sig\", \"phn_encoded_target\"],\n",
    "    )\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        [valid_data, test_data],\n",
    "        [\"id\", \"sig\", \"phn_encoded_target\", \"phn_encoded_canonical\", \"phn_encoded_perceived\"],\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_data, label_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating data loaders"
   ],
   "metadata": {
    "id": "6XqW25AMHGdp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVsbqzMyJBRL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "452dbf63-6636-4e54-c8c6-d36cc2d62c51"
   },
   "outputs": [],
   "source": [
    "hparams_file = '/content/drive/MyDrive/CS5647_Project/wave2vec2/hparams/train_wo_annealing.yaml'\n",
    "\n",
    "# Load hyperparameters file with command-line overrides\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)\n",
    "\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    ")\n",
    "\n",
    "# Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
    "train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hparams"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT1Avsi6r2sg",
    "outputId": "d089eb9e-49da-47eb-8a0f-c0bd2652ece6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initializing the ASR Trainer"
   ],
   "metadata": {
    "id": "vAXcj02HHSoY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQHyc36_74th",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0501378a-79fc-4048-e216-7cbeaf0633a8"
   },
   "outputs": [],
   "source": [
    "# Trainer initialization\n",
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    "    run_opts={\"device\": \"cuda\"}\n",
    ")\n",
    "asr_brain.label_encoder = label_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the ASR Model"
   ],
   "metadata": {
    "id": "F4IBOe31HZFG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Training/validation loop\n",
    "asr_brain.fit(\n",
    "    asr_brain.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKrrmNb0Hbjk",
    "outputId": "4cfe0c7c-f7e2-4dfb-9fee-fdf58cdd786a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluting the ASR Model"
   ],
   "metadata": {
    "id": "lyMtWR9XHb9p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test\n",
    "asr_brain.evaluate(\n",
    "    test_data,\n",
    "    min_key=\"PER\",\n",
    "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2ZHbh1cHgI_",
    "outputId": "ca86059e-138f-4c03-bfab-f71a3d8de568"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making inference on a single audio file"
   ],
   "metadata": {
    "id": "uPHWzXG9LH5m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_audio_path = \"/content/drive/MyDrive/CS5647_Project/dataset/TNI/wav/arctic_a0100.wav\"\n",
    "canonical_phonemes = \"sil y uw m ah s t s l iy p sil hh iy er jh d sil sil\" # actual sentence is 'You must sleep he urged'\n",
    "predicted_phonemes, score, stats = asr_brain.evaluate_test_audio(test_audio_path, canonical_phonemes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCTcRymBLLlh",
    "outputId": "9bed38ee-3379-48b0-a7bc-a4045cee4684"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "score"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFgYsw0yhnsQ",
    "outputId": "51f97666-8302-40a7-99ab-12f398c7b089"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predicted_phonemes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjWbnlzLhouI",
    "outputId": "397aeb40-87bb-4004-d319-bd622e382190"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "stats"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ih2SdwEJhqW7",
    "outputId": "d2d0231a-cb14-4fa1-d38d-5d767752c045"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
