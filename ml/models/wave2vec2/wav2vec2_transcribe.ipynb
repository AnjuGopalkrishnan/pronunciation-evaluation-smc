{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install speechbrain"],"metadata":{"id":"Q8cVpScymWde","executionInfo":{"status":"ok","timestamp":1697719353018,"user_tz":-480,"elapsed":12535,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"33693df2-853c-45d9-f6bc-57ebc2345d95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting speechbrain\n","  Downloading speechbrain-0.5.15-py3-none-any.whl (553 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.8/553.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperpyyaml (from speechbrain)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (23.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.11.3)\n","Collecting sentencepiece (from speechbrain)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.1+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.2+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.1)\n","Collecting huggingface-hub (from speechbrain)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (3.27.7)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (17.0.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.1)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml-0.17.36-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n","Installing collected packages: sentencepiece, ruamel.yaml.clib, ruamel.yaml, huggingface-hub, hyperpyyaml, speechbrain\n","Successfully installed huggingface-hub-0.18.0 hyperpyyaml-1.2.2 ruamel.yaml-0.17.36 ruamel.yaml.clib-0.2.8 sentencepiece-0.1.99 speechbrain-0.5.15\n"]}]},{"cell_type":"code","source":["!pip install textgrid transformers librosa"],"metadata":{"id":"YMmUc5DEmYvo","executionInfo":{"status":"ok","timestamp":1697719392236,"user_tz":-480,"elapsed":15228,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b509ef24-7a5a-4a14-9f77-b874941cbe02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textgrid\n","  Downloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\n","Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Installing collected packages: textgrid, safetensors, huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.18.0\n","    Uninstalling huggingface-hub-0.18.0:\n","      Successfully uninstalled huggingface-hub-0.18.0\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 textgrid-1.5 tokenizers-0.14.1 transformers-4.34.1\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","import torch\n","import logging\n","import speechbrain as sb\n","from hyperpyyaml import load_hyperpyyaml\n","import librosa\n","from tqdm import tqdm\n","import json\n","from google.colab import drive, files\n","import pandas as pd"],"metadata":{"id":"QVB9OjnnmEIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"3Mz1HH6LmFSV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697719424452,"user_tz":-480,"elapsed":23943,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"f070deb9-ec86-4166-979c-6fd93fd9a2d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["logger = logging.getLogger(__name__)"],"metadata":{"id":"smG8tugVmJGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder_path = '/content/drive/MyDrive/CS5647_Project'\n","os.chdir(folder_path)\n","current_directory = os.getcwd()\n","print(\"Current Working Directory after change:\", current_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snG6icFTI0zH","executionInfo":{"status":"ok","timestamp":1697719424453,"user_tz":-480,"elapsed":7,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"13b70159-dcef-4ba4-89b9-24cc35837643"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory after change: /content/drive/MyDrive/CS5647_Project\n"]}]},{"cell_type":"code","source":["def make_attn_mask(wavs, wav_lens):\n","    \"\"\"\n","    wav_lens: relative lengths(i.e. 0-1) of a batch. shape: (bs, )\n","    return a tensor of shape (bs, seq_len), representing mask on allowed positions.\n","            1 for regular tokens, 0 for padded tokens\n","    \"\"\"\n","    abs_lens = (wav_lens*wavs.shape[1]).long()\n","    attn_mask = wavs.new(wavs.shape).zero_().long()\n","    for i in range(len(abs_lens)):\n","        attn_mask[i, :abs_lens[i]] = 1\n","    return attn_mask"],"metadata":{"id":"AKOvFhBRmiF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGDCjpQDl886"},"outputs":[],"source":["# Define training procedure\n","class ASR(sb.Brain):\n","    def compute_forward(self, batch, stage):\n","        \"Given an input batch it computes the phoneme probabilities.\"\n","        batch = batch.to(self.device)\n","        ids = batch.id\n","        wavs, wav_lens = batch.sig\n","\n","        if stage == sb.Stage.TRAIN:\n","            if hasattr(self.hparams, \"augmentation\"):\n","                wavs = self.hparams.augmentation(wavs, wav_lens)\n","\n","        # some wav2vec models (e.g. large-lv60) needs attention_mask\n","        if self.modules.wav2vec2.feature_extractor.return_attention_mask:\n","            attn_mask = make_attn_mask(wavs, wav_lens)\n","            feats = self.modules.wav2vec2(wavs, attention_mask=attn_mask)\n","        else:\n","            attn_mask = None\n","            feats = self.modules.wav2vec2(wavs)\n","\n","        x = self.modules.enc(feats)\n","\n","        # output layer for ctc log-probabilities\n","        logits = self.modules.ctc_lin(x)\n","        p_ctc = self.hparams.log_softmax(logits)\n","        # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n","        # that is, it return a list of list with different lengths\n","        sequence = sb.decoders.ctc_greedy_decode(\n","            p_ctc, wav_lens, blank_id=self.hparams.blank_index\n","        )\n","        transcriptions = [\" \".join(self.label_encoder.decode_ndim(s)) for s in sequence]\n","\n","\n","        return ids, transcriptions\n","\n","    def transcribe_dataset(\n","            self,\n","            dataset, # Must be obtained from the dataio_function\n","            min_key, # We load the model with the lowest WER\n","            loader_kwargs # opts for the dataloading\n","        ):\n","\n","        # If dataset isn't a Dataloader, we create it.\n","        if not isinstance(dataset, torch.utils.data.DataLoader):\n","            loader_kwargs[\"ckpt_prefix\"] = None\n","            dataset = self.make_dataloader(\n","                dataset, sb.Stage.TEST, **loader_kwargs\n","            )\n","\n","\n","        self.on_evaluate_start(min_key=min_key) # We call the on_evaluate_start that will load the best model\n","        self.modules.eval() # We set the model to eval mode (remove dropout etc)\n","        self.modules.wav2vec2.model.config.apply_spec_augment = False  # make sure no spec aug applied on wav2vec2\n","\n","        # Now we iterate over the dataset and we simply compute_forward and decode\n","        with torch.no_grad():\n","\n","            wav_ids = []\n","            transcripts = []\n","            for batch in tqdm(dataset, dynamic_ncols=True):\n","\n","                ids, preds = self.compute_forward(batch, stage=sb.Stage.TEST)\n","\n","                transcripts.extend(preds)\n","                wav_ids.extend(ids)\n","\n","        return wav_ids, transcripts\n","\n","\n","def dataio_prep(hparams):\n","    \"\"\"This function prepares the datasets to be used in the brain class.\n","    It also defines the data processing pipeline through user-defined functions.\"\"\"\n","    data_folder = hparams[\"data_folder_save\"]\n","    # 1. Declarations:\n","\n","    inference_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n","        csv_path=hparams[\"inference_annotation\"],\n","        replacements={\"data_root\": data_folder},\n","    )\n","    inference_data = inference_data.filtered_sorted(sort_key=\"duration\")\n","\n","    datasets = [inference_data]\n","    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n","\n","    # 2. Define audio pipeline:\n","    @sb.utils.data_pipeline.takes(\"wav\")\n","    @sb.utils.data_pipeline.provides(\"sig\")\n","    def audio_pipeline(wav):\n","        # sig = sb.dataio.dataio.read_audio(wav)\n","        # # sample rate change to 16000, e,g, using librosa\n","        # sig = torch.Tensor(librosa.core.load(wav, hparams[\"sample_rate\"])[0])\n","        # Use wav2vec processor to do normalization\n","        audio_signal, _ = librosa.core.load(wav, sr=hparams[\"sample_rate\"])\n","        sig = hparams[\"wav2vec2\"].feature_extractor(\n","            audio_signal,\n","            sampling_rate=hparams[\"sample_rate\"],\n","        ).input_values[0]\n","        sig = torch.Tensor(sig)\n","        return sig\n","\n","    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n","\n","\n","    # 3. Fit encoder:\n","    # Load the label encoder\n","    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n","    label_encoder.load(lab_enc_file)\n","\n","    # 4. Set output:\n","    sb.dataio.dataset.set_output_keys(\n","        datasets,\n","        [\"id\", \"sig\"],\n","    )\n","\n","    return inference_data, label_encoder"]},{"cell_type":"code","source":["hparams_file = '/content/drive/MyDrive/CS5647_Project/hparams/transcribe.yaml'\n","\n","# Load hyperparameters file with command-line overrides\n","with open(hparams_file) as fin:\n","    hparams = load_hyperpyyaml(fin)\n","\n"],"metadata":{"id":"mB9LrTAVmpb5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697719444350,"user_tz":-480,"elapsed":19901,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"55932e16-b9b7-4369-ce68-1ae521e8ce32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n","WARNING:speechbrain.lobes.models.huggingface_wav2vec:speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n"]}]},{"cell_type":"code","source":["\n","# Create experiment directory\n","sb.create_experiment_directory(\n","    experiment_directory=hparams[\"output_folder\"],\n","    hyperparams_to_save=hparams_file,\n",")\n","\n","\n","# # Dataset IO prep: creating Dataset objects and proper encodings for phones\n","inference_data, label_encoder = dataio_prep(hparams)\n","\n","# Trainer initialization\n","asr_brain = ASR(\n","    modules=hparams[\"modules\"],\n","    hparams=hparams,\n","    checkpointer=hparams[\"checkpointer\"],\n",")\n","asr_brain.label_encoder = label_encoder\n","wav_ids, transcripts = asr_brain.transcribe_dataset(\n","    dataset=inference_data, # Must be obtained from the dataio_function\n","    min_key=\"PER\", # We load the model with the lowest PER\n","    loader_kwargs=hparams[\"inference_dataloader_opts\"], # opts for the dataloading\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYTD-ObAKL_w","executionInfo":{"status":"ok","timestamp":1697719731046,"user_tz":-480,"elapsed":286714,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"3fe7492f-bfec-4db4-eca4-80328e57eb82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["speechbrain.core - Beginning experiment!\n","speechbrain.core - Experiment folder: results/wav2vec2-base_ctc\n","speechbrain.core - 90.6M trainable parameters in ASR\n","speechbrain.utils.checkpoints - Loading a checkpoint from results/wav2vec2-base_ctc/save/CKPT+2023-10-19+00-55-18+00\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n","  warnings.warn(\n","100%|██████████| 300/300 [04:27<00:00,  1.12it/s]\n"]}]},{"cell_type":"code","source":["import csv\n","import json\n","\n","# Read data from the CSV file\n","csv_data = {}\n","with open(hparams[\"inference_annotation\"], \"r\") as csv_file:\n","    csv_reader = csv.DictReader(csv_file)\n","    for row in csv_reader:\n","        csv_data[row[\"ID\"]] = row\n","\n","# Update pred_phns based on wav_ids and transcripts\n","for wav_id, transcript in zip(wav_ids, transcripts):\n","    if wav_id in csv_data:\n","        csv_data[wav_id][\"pred_phns\"] = transcript\n","\n","# Convert the updated data to the desired format\n","updated_data = {wav_id: csv_data[wav_id] for wav_id in csv_data}\n","\n","# Save the updated data as a new JSON file\n","with open(hparams[\"inference_annotation_saved\"], \"w\") as json_f_save:\n","    json.dump(updated_data, json_f_save, indent=2)"],"metadata":{"id":"lyVyR4pbZQHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cixnvqCFK07K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O5GPwk3RXC5F"},"execution_count":null,"outputs":[]}]}