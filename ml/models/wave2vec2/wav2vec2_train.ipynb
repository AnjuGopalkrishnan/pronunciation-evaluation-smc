{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing required packages"
   ],
   "metadata": {
    "id": "sWUkD-DTGaD7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kWkjs_8PHcBf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fbcf90be-1e20-4aa9-94ba-738dd5c4712b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting speechbrain\n",
      "  Downloading speechbrain-0.5.15-py3-none-any.whl (553 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m553.8/553.8 kB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting hyperpyyaml (from speechbrain)\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.3.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (23.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.11.3)\n",
      "Collecting sentencepiece (from speechbrain)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m19.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.19.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.1)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.4/116.4 kB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m526.7/526.7 kB\u001B[0m \u001B[31m26.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n",
      "Installing collected packages: sentencepiece, ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
      "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 sentencepiece-0.1.99 speechbrain-0.5.15\n",
      "Collecting textgrid\n",
      "  Downloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Installing collected packages: textgrid\n",
      "Successfully installed textgrid-1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install speechbrain\n",
    "!pip install textgrid transformers librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing requried modules"
   ],
   "metadata": {
    "id": "Xn1PV5KnGeN2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "53nh8Xl--eZ4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import librosa\n",
    "import csv\n",
    "from google.colab import drive, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mounting google drive"
   ],
   "metadata": {
    "id": "ZCERYWlXGkTd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kiA9SjWIHOx5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4cefff9e-df17-4600-8fe1-32b7bd5069d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Constants & Importing Eval metric file"
   ],
   "metadata": {
    "id": "tBhwzhbEG2gZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sikO-MV-Iv0V",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2b608934-280b-489e-a143-26cbbbb33361"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current Working Directory after change: /content/drive/MyDrive/CS5647_Project\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "folder_path = '/content/drive/MyDrive/CS5647_Project'\n",
    "os.chdir(folder_path)\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory after change:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i1KP_dzhN-A7"
   },
   "outputs": [],
   "source": [
    "from mpd_eval_v3 import MpdStats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Speech Brain object"
   ],
   "metadata": {
    "id": "QY3NauW8G-jU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P3meQIqYI1tr"
   },
   "outputs": [],
   "source": [
    "def make_attn_mask(wavs, wav_lens):\n",
    "    \"\"\"\n",
    "    wav_lens: relative lengths(i.e. 0-1) of a batch. shape: (bs, )\n",
    "    return a tensor of shape (bs, seq_len), representing mask on allowed positions.\n",
    "            1 for regular tokens, 0 for padded tokens\n",
    "    \"\"\"\n",
    "    abs_lens = (wav_lens*wavs.shape[1]).long()\n",
    "    attn_mask = wavs.new(wavs.shape).zero_().long()\n",
    "    for i in range(len(abs_lens)):\n",
    "        attn_mask[i, :abs_lens[i]] = 1\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MRjl6kGqI7Lr"
   },
   "outputs": [],
   "source": [
    "class ASR(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"Given an input batch it computes the phoneme probabilities.\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        # phns_bos, _ = batch.phn_encoded_bos\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
    "\n",
    "        # some wav2vec models (e.g. large-lv60) needs attention_mask\n",
    "        if self.modules.wav2vec2.feature_extractor.return_attention_mask:\n",
    "            attn_mask = make_attn_mask(wavs, wav_lens)\n",
    "            feats = self.modules.wav2vec2(wavs, attention_mask=attn_mask)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "            feats = self.modules.wav2vec2(wavs)\n",
    "\n",
    "        x = self.modules.enc(feats)\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(x)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "\n",
    "        return p_ctc, wav_lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"Given the network predictions and targets computed the NLL loss.\"\n",
    "\n",
    "        p_ctc, wav_lens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        targets, target_lens = batch.phn_encoded_target\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            canonicals, canonical_lens = batch.phn_encoded_canonical\n",
    "            perceiveds, perceived_lens = batch.phn_encoded_perceived\n",
    "\n",
    "        loss_ctc = self.hparams.ctc_cost(p_ctc, targets, wav_lens, target_lens)\n",
    "        loss = loss_ctc\n",
    "\n",
    "        # Record losses for posterity\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n",
    "            # that is, it return a list of list with different lengths\n",
    "            sequence = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "            self.ctc_metrics.append(ids, p_ctc, targets, wav_lens, target_lens)\n",
    "\n",
    "            self.per_metrics.append(\n",
    "                ids=ids,\n",
    "                predict=sequence,\n",
    "                target=targets,\n",
    "                predict_len=None,\n",
    "                target_len=target_lens,\n",
    "                ind2lab=self.label_encoder.decode_ndim,\n",
    "            )\n",
    "            self.mpd_metrics.append(\n",
    "                ids=ids,\n",
    "                predict=sequence,\n",
    "                canonical=canonicals,\n",
    "                perceived=perceiveds,\n",
    "                predict_len=None,\n",
    "                canonical_len=canonical_lens,\n",
    "                perceived_len=perceived_lens,\n",
    "                ind2lab=self.label_encoder.decode_ndim,\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        predictions = self.compute_forward(batch, stage=stage)\n",
    "        loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"Gets called when a stage (either training, validation, test) starts.\"\n",
    "        self.ctc_metrics = self.hparams.ctc_stats()\n",
    "        if self.hparams.wav2vec2_specaug:\n",
    "            self.modules.wav2vec2.model.config.apply_spec_augment = True\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.modules.wav2vec2.model.config.apply_spec_augment = False\n",
    "            self.per_metrics = self.hparams.per_stats()\n",
    "            self.mpd_metrics = MpdStats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "        else:\n",
    "            per = self.per_metrics.summarize(\"error_rate\")\n",
    "            mpd_f1 = self.mpd_metrics.summarize(\"mpd_f1\")\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"lr_adam\": self.adam_optimizer.param_groups[0][\"lr\"],\n",
    "                    \"lr_wav2vec\": self.wav2vec_optimizer.param_groups[0][\"lr\"],\n",
    "                },\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats={\n",
    "                    \"loss\": stage_loss,\n",
    "                    \"ctc_loss\": self.ctc_metrics.summarize(\"average\"),\n",
    "                    \"PER\": per,\n",
    "                    \"mpd_f1\": mpd_f1\n",
    "                },\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"PER\": per, \"mpd_f1\": mpd_f1}, min_keys=[\"PER\"], max_keys=[\"mpd_f1\"]\n",
    "            )\n",
    "\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats={\"loss\": stage_loss, \"PER\": per, \"mpd_f1\": mpd_f1},\n",
    "            )\n",
    "            with open(self.hparams.wer_file, \"w\") as w:\n",
    "                w.write(\"CTC loss stats:\\n\")\n",
    "                self.ctc_metrics.write_stats(w)\n",
    "                w.write(\"\\nPER stats:\\n\")\n",
    "                self.per_metrics.write_stats(w)\n",
    "                print(\n",
    "                    \"CTC and PER stats written to file\",\n",
    "                    self.hparams.wer_file,\n",
    "                )\n",
    "            with open(self.hparams.mpd_file, \"w\") as m:\n",
    "                m.write(\"MPD results and stats:\\n\")\n",
    "                self.mpd_metrics.write_stats(m)\n",
    "                print(\n",
    "                    \"MPD results and stats written to file\",\n",
    "                    self.hparams.mpd_file,\n",
    "                )\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Fit one batch, override to do multiple updates.\n",
    "\n",
    "        The default implementation depends on a few methods being defined\n",
    "        with a particular behavior:\n",
    "\n",
    "        * ``compute_forward()``\n",
    "        * ``compute_objectives()``\n",
    "\n",
    "        Also depends on having optimizers passed at initialization.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : list of torch.Tensors\n",
    "            Batch of data to use for training. Default implementation assumes\n",
    "            this batch has two elements: inputs and targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        detached loss\n",
    "        \"\"\"\n",
    "        # Managing automatic mixed precision\n",
    "        if self.auto_mix_prec:\n",
    "\n",
    "            self.wav2vec_optimizer.zero_grad()\n",
    "            self.adam_optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.wav2vec_optimizer)\n",
    "            self.scaler.unscale_(self.adam_optimizer)\n",
    "\n",
    "            if self.check_gradients(loss):\n",
    "                self.scaler.step(self.wav2vec_optimizer)\n",
    "                self.scaler.step(self.adam_optimizer)\n",
    "\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "\n",
    "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "            # normalize the loss by gradient_accumulation step\n",
    "            (loss / self.hparams.gradient_accumulation).backward()\n",
    "\n",
    "            if self.step % self.hparams.gradient_accumulation == 0:\n",
    "                # gradient clipping & early stop if loss is not fini\n",
    "                if self.check_gradients(loss):\n",
    "                    self.wav2vec_optimizer.step()\n",
    "                    self.adam_optimizer.step()\n",
    "\n",
    "                self.wav2vec_optimizer.zero_grad()\n",
    "                self.adam_optimizer.zero_grad()\n",
    "\n",
    "        return loss.detach().cpu()\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
    "        self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "            self.modules.wav2vec2.model.parameters()\n",
    "        )\n",
    "        self.adam_optimizer = self.hparams.adam_opt_class(\n",
    "            self.hparams.model.parameters()\n",
    "        )\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec_opt\", self.wav2vec_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"adam_opt\", self.adam_optimizer)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Gets called at the beginning of ``fit()``, on multiple processes\n",
    "        if ``distributed_count > 0`` and backend is ddp.\n",
    "\n",
    "        Default implementation compiles the jit modules, initializes\n",
    "        optimizers, and loads the latest checkpoint to resume training.\n",
    "        \"\"\"\n",
    "        # Run this *after* starting all processes since jit modules cannot be\n",
    "        # pickled.\n",
    "        self._compile_jit()\n",
    "\n",
    "        # Wrap modules with parallel backend after jit\n",
    "        self._wrap_distributed()\n",
    "\n",
    "        # Initialize optimizers after parameters are configured\n",
    "        self.init_optimizers()\n",
    "\n",
    "        # Load latest checkpoint to resume training if interrupted\n",
    "        ## NOTE: make sure to use the \"best\" model to continual training\n",
    "        ## so we set the `min_key` argument\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.recover_if_possible(\n",
    "                device=torch.device(self.device),\n",
    "                min_key=\"PER\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Brain Data preparation"
   ],
   "metadata": {
    "id": "dkrT2roeHHwZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nfd0QdzCeDuX"
   },
   "outputs": [],
   "source": [
    "def dataio_prep(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = hparams[\"data_folder_save\"]\n",
    "    # 1. Declarations:\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        # we sort training data to speed up training and get better results.\n",
    "        train_data = train_data.filtered_sorted(sort_key=\"duration\")\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"test_annotation\"],\n",
    "        replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    test_data = test_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    datasets = [train_data, valid_data, test_data]\n",
    "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
    "\n",
    "    # 2. Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        # sig = sb.dataio.dataio.read_audio(wav)\n",
    "        # # sample rate change to 16000, e,g, using librosa\n",
    "        # sig = torch.Tensor(librosa.core.load(wav, hparams[\"sample_rate\"])[0])\n",
    "        # Use wav2vec processor to do normalization\n",
    "        sig = hparams[\"wav2vec2\"].feature_extractor(\n",
    "            librosa.core.load(wav, sr=hparams[\"sample_rate\"])[0],\n",
    "            sampling_rate=hparams[\"sample_rate\"],\n",
    "        ).input_values[0]\n",
    "        sig = torch.Tensor(sig)\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "    # 3. Define text pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"perceived_train_target\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"phn_list_target\",\n",
    "        \"phn_encoded_list_target\",\n",
    "        \"phn_encoded_target\",\n",
    "    )\n",
    "    def text_pipeline_train(phn):\n",
    "        phn_list = phn.strip().split()\n",
    "        yield phn_list\n",
    "        phn_encoded_list = label_encoder.encode_sequence(phn_list)\n",
    "        yield phn_encoded_list\n",
    "        phn_encoded = torch.LongTensor(phn_encoded_list)\n",
    "        yield phn_encoded\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"perceived_train_target\", \"canonical_aligned\", \"perceived_aligned\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"phn_list_target\",\n",
    "        \"phn_encoded_list_target\",\n",
    "        \"phn_encoded_target\",\n",
    "        \"phn_list_canonical\",\n",
    "        \"phn_encoded_list_canonical\",\n",
    "        \"phn_encoded_canonical\",\n",
    "        \"phn_list_perceived\",\n",
    "        \"phn_encoded_list_perceived\",\n",
    "        \"phn_encoded_perceived\",\n",
    "    )\n",
    "    def text_pipeline_test(target, canonical, perceived):\n",
    "        phn_list_target = target.strip().split()\n",
    "        yield phn_list_target\n",
    "        phn_encoded_list_target = label_encoder.encode_sequence(phn_list_target)\n",
    "        yield phn_encoded_list_target\n",
    "        phn_encoded_target = torch.LongTensor(phn_encoded_list_target)\n",
    "        yield phn_encoded_target\n",
    "        phn_list_canonical = canonical.strip().split()\n",
    "        yield phn_list_canonical\n",
    "        phn_encoded_list_canonical = label_encoder.encode_sequence(phn_list_canonical)\n",
    "        yield phn_encoded_list_canonical\n",
    "        phn_encoded_canonical = torch.LongTensor(phn_encoded_list_canonical)\n",
    "        yield phn_encoded_canonical\n",
    "        phn_list_perceived = perceived.strip().split()\n",
    "        yield phn_list_perceived\n",
    "        phn_encoded_list_perceived = label_encoder.encode_sequence(phn_list_perceived)\n",
    "        yield phn_encoded_list_perceived\n",
    "        phn_encoded_perceived = torch.LongTensor(phn_encoded_list_perceived)\n",
    "        yield phn_encoded_perceived\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item([train_data], text_pipeline_train)\n",
    "    sb.dataio.dataset.add_dynamic_item([valid_data, test_data], text_pipeline_test)\n",
    "\n",
    "    # 3. Fit encoder:\n",
    "    # Load or compute the label encoder\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    special_labels = {\n",
    "        \"blank_label\": hparams[\"blank_index\"],\n",
    "    }\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[train_data],\n",
    "        output_key=\"phn_list_target\",\n",
    "        special_labels=special_labels,\n",
    "        sequence_input=True,\n",
    "    )\n",
    "\n",
    "    # 4. Set output:\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        [train_data],\n",
    "        [\"id\", \"sig\", \"phn_encoded_target\"],\n",
    "    )\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        [valid_data, test_data],\n",
    "        [\"id\", \"sig\", \"phn_encoded_target\", \"phn_encoded_canonical\", \"phn_encoded_perceived\"],\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_data, label_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating data loaders"
   ],
   "metadata": {
    "id": "6XqW25AMHGdp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yVsbqzMyJBRL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "561c0bbc-cfb1-4ded-86e3-0d1c8d1fc545"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:381: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/wav2vec2-base_ctc/\n",
      "speechbrain.dataio.encoder - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n"
     ]
    }
   ],
   "source": [
    "hparams_file = '/content/drive/MyDrive/CS5647_Project/wave2vec2/hparams/train.yaml'\n",
    "\n",
    "# Load hyperparameters file with command-line overrides\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)\n",
    "\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    ")\n",
    "\n",
    "# Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
    "train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hparams"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNYOXMZpIHpR",
    "outputId": "44c6c02b-5186-4973-a414-065db105ce34"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'seed': 1234,\n",
       " 'output_folder': 'results/wav2vec2-base_ctc/',\n",
       " 'wer_file': 'results/wav2vec2-base_ctc//wer.txt',\n",
       " 'mpd_file': 'results/wav2vec2-base_ctc//mpd.txt',\n",
       " 'save_folder': 'results/wav2vec2-base_ctc//save',\n",
       " 'train_log': 'results/wav2vec2-base_ctc//train_log.txt',\n",
       " 'wav2vec2_hub': 'facebook/wav2vec2-base',\n",
       " 'data_folder_save': './data',\n",
       " 'train_annotation': './data/train_data.csv',\n",
       " 'valid_annotation': './data/val_data.csv',\n",
       " 'test_annotation': './data/test_data.csv',\n",
       " 'number_of_epochs': 100,\n",
       " 'batch_size': 8,\n",
       " 'num_workers': 4,\n",
       " 'lr': 0.0003,\n",
       " 'lr_wav2vec': 1e-05,\n",
       " 'sorting': 'ascending',\n",
       " 'auto_mix_prec': False,\n",
       " 'sample_rate': 16000,\n",
       " 'gradient_accumulation': 2,\n",
       " 'activation': torch.nn.modules.activation.LeakyReLU,\n",
       " 'dnn_layers': 2,\n",
       " 'dnn_neurons': 384,\n",
       " 'freeze_wav2vec': False,\n",
       " 'freeze_wav2vec_feature_extractor': True,\n",
       " 'wav2vec2_specaug': True,\n",
       " 'output_neurons': 42,\n",
       " 'blank_index': 0,\n",
       " 'train_dataloader_opts': {'batch_size': 8,\n",
       "  'num_workers': 4,\n",
       "  'shuffle': False},\n",
       " 'valid_dataloader_opts': {'batch_size': 8, 'num_workers': 1},\n",
       " 'test_dataloader_opts': {'batch_size': 1, 'num_workers': 1},\n",
       " 'augmentation': TimeDomainSpecAugment(\n",
       "   (speed_perturb): SpeedPerturb()\n",
       "   (drop_freq): DropFreq()\n",
       "   (drop_chunk): DropChunk()\n",
       " ),\n",
       " 'epoch_counter': <speechbrain.utils.epoch_loop.EpochCounter at 0x7e0c7dd20940>,\n",
       " 'enc': VanillaNN(\n",
       "   (linear): Linear(\n",
       "     (w): Linear(in_features=768, out_features=384, bias=True)\n",
       "   )\n",
       "   (act): LeakyReLU(negative_slope=0.01)\n",
       "   (linear_0): Linear(\n",
       "     (w): Linear(in_features=384, out_features=384, bias=True)\n",
       "   )\n",
       "   (act_0): LeakyReLU(negative_slope=0.01)\n",
       " ),\n",
       " 'wav2vec2': HuggingFaceWav2Vec2(\n",
       "   (model): Wav2Vec2Model(\n",
       "     (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "       (conv_layers): ModuleList(\n",
       "         (0): Wav2Vec2GroupNormConvLayer(\n",
       "           (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "           (activation): GELUActivation()\n",
       "           (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "         )\n",
       "         (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "           (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "           (activation): GELUActivation()\n",
       "         )\n",
       "         (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "           (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "           (activation): GELUActivation()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (feature_projection): Wav2Vec2FeatureProjection(\n",
       "       (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): Wav2Vec2Encoder(\n",
       "       (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "         (conv): ParametrizedConv1d(\n",
       "           768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "           (parametrizations): ModuleDict(\n",
       "             (weight): ParametrizationList(\n",
       "               (0): _WeightNorm()\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (padding): Wav2Vec2SamePadLayer()\n",
       "         (activation): GELUActivation()\n",
       "       )\n",
       "       (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "           (attention): Wav2Vec2Attention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (feed_forward): Wav2Vec2FeedForward(\n",
       "             (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "             (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "             (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'ctc_lin': Linear(\n",
       "   (w): Linear(in_features=384, out_features=42, bias=True)\n",
       " ),\n",
       " 'log_softmax': Softmax(\n",
       "   (act): LogSoftmax(dim=-1)\n",
       " ),\n",
       " 'ctc_cost': functools.partial(<function ctc_loss at 0x7e0c593272e0>, blank_index=0),\n",
       " 'model': ModuleList(\n",
       "   (0): VanillaNN(\n",
       "     (linear): Linear(\n",
       "       (w): Linear(in_features=768, out_features=384, bias=True)\n",
       "     )\n",
       "     (act): LeakyReLU(negative_slope=0.01)\n",
       "     (linear_0): Linear(\n",
       "       (w): Linear(in_features=384, out_features=384, bias=True)\n",
       "     )\n",
       "     (act_0): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (1): Linear(\n",
       "     (w): Linear(in_features=384, out_features=42, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'adam_opt_class': functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0003),\n",
       " 'wav2vec_opt_class': functools.partial(<class 'torch.optim.adam.Adam'>, lr=1e-05),\n",
       " 'modules': {'wav2vec2': HuggingFaceWav2Vec2(\n",
       "    (model): Wav2Vec2Model(\n",
       "      (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Wav2Vec2GroupNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): Wav2Vec2FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): Wav2Vec2Encoder(\n",
       "        (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (padding): Wav2Vec2SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "            (attention): Wav2Vec2Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): Wav2Vec2FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'enc': VanillaNN(\n",
       "    (linear): Linear(\n",
       "      (w): Linear(in_features=768, out_features=384, bias=True)\n",
       "    )\n",
       "    (act): LeakyReLU(negative_slope=0.01)\n",
       "    (linear_0): Linear(\n",
       "      (w): Linear(in_features=384, out_features=384, bias=True)\n",
       "    )\n",
       "    (act_0): LeakyReLU(negative_slope=0.01)\n",
       "  ),\n",
       "  'ctc_lin': Linear(\n",
       "    (w): Linear(in_features=384, out_features=42, bias=True)\n",
       "  )},\n",
       " 'checkpointer': <speechbrain.utils.checkpoints.Checkpointer at 0x7e0dbac43130>,\n",
       " 'train_logger': <speechbrain.utils.train_logger.FileTrainLogger at 0x7e0dbac41750>,\n",
       " 'ctc_stats': functools.partial(<class 'speechbrain.utils.metric_stats.MetricStats'>, metric=functools.partial(<function ctc_loss at 0x7e0c593272e0>, blank_index=0, reduction='batch')),\n",
       " 'per_stats': speechbrain.utils.metric_stats.ErrorRateStats}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initializing the ASR Trainer"
   ],
   "metadata": {
    "id": "vAXcj02HHSoY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QQHyc36_74th",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d8423279-16d3-4007-c5f4-35557fad7ad3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.core - Info: auto_mix_prec arg from hparam file is used\n",
      "speechbrain.core - 90.6M trainable parameters in ASR\n"
     ]
    }
   ],
   "source": [
    "# Trainer initialization\n",
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    "    run_opts={\"device\": \"cuda\"}\n",
    ")\n",
    "asr_brain.label_encoder = label_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the ASR Model"
   ],
   "metadata": {
    "id": "F4IBOe31HZFG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Training/validation loop\n",
    "asr_brain.fit(\n",
    "    asr_brain.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "    valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKrrmNb0Hbjk",
    "outputId": "298d8acf-bf57-4a76-a3ae-cb81c2cc7890"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/wav2vec2-base_ctc/save/CKPT+2023-11-21+13-48-03+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 3\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/75 [00:03<?, ?it/s]/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 75/75 [00:19<00:00,  3.75it/s, train_loss=3.42]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/38 [00:03<?, ?it/s]/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 38/38 [00:06<00:00,  5.96it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.utils.train_logger - epoch: 3, lr_adam: 3.00e-04, lr_wav2vec: 1.00e-05 - train loss: 3.42 - valid loss: 3.56, valid ctc_loss: 3.56, valid PER: 99.30, valid mpd_f1: 2.00e-01\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2-base_ctc/save/CKPT+2023-11-21+13-50-28+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2-base_ctc/save/CKPT+2023-11-21+13-48-03+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 75/75 [00:20<00:00,  3.60it/s, train_loss=3.37]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 38/38 [00:07<00:00,  5.36it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.utils.train_logger - epoch: 4, lr_adam: 3.00e-04, lr_wav2vec: 1.00e-05 - train loss: 3.37 - valid loss: 3.40, valid ctc_loss: 3.40, valid PER: 95.49, valid mpd_f1: 2.00e-01\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2-base_ctc/save/CKPT+2023-11-21+13-50-59+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2-base_ctc/save/CKPT+2023-11-21+13-50-28+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluting the ASR Model"
   ],
   "metadata": {
    "id": "lyMtWR9XHb9p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test\n",
    "asr_brain.evaluate(\n",
    "    test_data,\n",
    "    min_key=\"PER\",\n",
    "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    ")"
   ],
   "metadata": {
    "id": "z2ZHbh1cHgI_"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuType": "V100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
