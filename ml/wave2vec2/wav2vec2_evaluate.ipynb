{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","cell_execution_strategy":"setup"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install speechbrain"],"metadata":{"id":"BHahOlF-dP_L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697714447247,"user_tz":-480,"elapsed":7055,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"464eded5-901a-4c95-8630-240a026effb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting speechbrain\n","  Downloading speechbrain-0.5.15-py3-none-any.whl (553 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/553.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/553.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m553.0/553.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.8/553.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperpyyaml (from speechbrain)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (23.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.11.3)\n","Collecting sentencepiece (from speechbrain)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.1+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.2+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.1)\n","Collecting huggingface-hub (from speechbrain)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (3.27.7)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (17.0.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.1)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml-0.17.36-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n","Installing collected packages: sentencepiece, ruamel.yaml.clib, ruamel.yaml, huggingface-hub, hyperpyyaml, speechbrain\n","Successfully installed huggingface-hub-0.18.0 hyperpyyaml-1.2.2 ruamel.yaml-0.17.36 ruamel.yaml.clib-0.2.8 sentencepiece-0.1.99 speechbrain-0.5.15\n"]}]},{"cell_type":"code","source":["!pip install textgrid transformers librosa"],"metadata":{"id":"myIl824AdTxf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697714458852,"user_tz":-480,"elapsed":11610,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"ddacdc90-c419-496a-e96e-46a16e964649"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textgrid\n","  Downloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\n","Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Installing collected packages: textgrid, safetensors, huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.18.0\n","    Uninstalling huggingface-hub-0.18.0:\n","      Successfully uninstalled huggingface-hub-0.18.0\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 textgrid-1.5 tokenizers-0.14.1 transformers-4.34.1\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","import torch\n","import logging\n","import speechbrain as sb\n","from hyperpyyaml import load_hyperpyyaml\n","import librosa\n","import json\n","from google.colab import drive, files"],"metadata":{"id":"siWciZfxdNo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"9-zu3c33euEQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697714488510,"user_tz":-480,"elapsed":23367,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"535bac1c-eb83-476d-db22-946cb4bb01c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["logger = logging.getLogger(__name__)"],"metadata":{"id":"X4eQtbbcev1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder_path = '/content/drive/MyDrive/CS5647_Project'\n","os.chdir(folder_path)\n","current_directory = os.getcwd()\n","print(\"Current Working Directory after change:\", current_directory)"],"metadata":{"id":"wKFk1wM2ezwQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697714488511,"user_tz":-480,"elapsed":6,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"4cbf075f-b305-4d9a-a93c-8a676bfd671d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory after change: /content/drive/MyDrive/CS5647_Project\n"]}]},{"cell_type":"code","source":["from mpd_eval_v3 import MpdStats"],"metadata":{"id":"QrX-pSFge1m3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_attn_mask(wavs, wav_lens):\n","    \"\"\"\n","    wav_lens: relative lengths(i.e. 0-1) of a batch. shape: (bs, )\n","    return a tensor of shape (bs, seq_len), representing mask on allowed positions.\n","            1 for regular tokens, 0 for padded tokens\n","    \"\"\"\n","    abs_lens = (wav_lens*wavs.shape[1]).long()\n","    attn_mask = wavs.new(wavs.shape).zero_().long()\n","    for i in range(len(abs_lens)):\n","        attn_mask[i, :abs_lens[i]] = 1\n","    return attn_mask"],"metadata":{"id":"vsgn89d9e4bf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGYM0WIzW8G3"},"outputs":[],"source":["# Define training procedure\n","class ASR(sb.Brain):\n","    def compute_forward(self, batch, stage):\n","        \"Given an input batch it computes the phoneme probabilities.\"\n","        batch = batch.to(self.device)\n","        wavs, wav_lens = batch.sig\n","        # phns_bos, _ = batch.phn_encoded_bos\n","\n","        if stage == sb.Stage.TRAIN:\n","            if hasattr(self.hparams, \"augmentation\"):\n","                wavs = self.hparams.augmentation(wavs, wav_lens)\n","\n","        # some wav2vec models (e.g. large-lv60) needs attention_mask\n","        if self.modules.wav2vec2.feature_extractor.return_attention_mask:\n","            attn_mask = make_attn_mask(wavs, wav_lens)\n","            feats = self.modules.wav2vec2(wavs, attention_mask=attn_mask)\n","        else:\n","            attn_mask = None\n","            feats = self.modules.wav2vec2(wavs)\n","\n","        x = self.modules.enc(feats)\n","\n","        # output layer for ctc log-probabilities\n","        logits = self.modules.ctc_lin(x)\n","        p_ctc = self.hparams.log_softmax(logits)\n","\n","        return p_ctc, wav_lens\n","\n","    def compute_objectives(self, predictions, batch, stage):\n","        \"Given the network predictions and targets computed the NLL loss.\"\n","\n","        p_ctc, wav_lens = predictions\n","\n","        ids = batch.id\n","        # phns_eos, phn_lens_eos = batch.phn_encoded_eos\n","        targets, target_lens = batch.phn_encoded_target\n","        if stage != sb.Stage.TRAIN:\n","            canonicals, canonical_lens = batch.phn_encoded_canonical\n","            perceiveds, perceived_lens = batch.phn_encoded_perceived\n","\n","        loss_ctc = self.hparams.ctc_cost(p_ctc, targets, wav_lens, target_lens)\n","        loss = loss_ctc\n","\n","        # Record losses for posterity\n","        if stage != sb.Stage.TRAIN:\n","            # Note: sb.decoders.ctc_greedy_decode will also remove padded tokens\n","            # that is, it return a list of list with different lengths\n","            sequence = sb.decoders.ctc_greedy_decode(\n","                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n","            )\n","            self.ctc_metrics.append(ids, p_ctc, targets, wav_lens, target_lens)\n","\n","            self.per_metrics.append(\n","                ids=ids,\n","                predict=sequence,\n","                target=targets,\n","                predict_len=None,\n","                target_len=target_lens,\n","                ind2lab=self.label_encoder.decode_ndim,\n","            )\n","            self.mpd_metrics.append(\n","                ids=ids,\n","                predict=sequence,\n","                canonical=canonicals,\n","                perceived=perceiveds,\n","                predict_len=None,\n","                canonical_len=canonical_lens,\n","                perceived_len=perceived_lens,\n","                ind2lab=self.label_encoder.decode_ndim,\n","            )\n","\n","        return loss\n","\n","    def evaluate_batch(self, batch, stage):\n","        \"\"\"Computations needed for validation/test batches\"\"\"\n","        predictions = self.compute_forward(batch, stage=stage)\n","        loss = self.compute_objectives(predictions, batch, stage=stage)\n","        return loss.detach()\n","\n","    def on_stage_start(self, stage, epoch):\n","        \"Gets called when a stage (either training, validation, test) starts.\"\n","        self.ctc_metrics = self.hparams.ctc_stats()\n","        if self.hparams.wav2vec2_specaug:\n","            self.modules.wav2vec2.model.config.apply_spec_augment = True\n","\n","        if stage != sb.Stage.TRAIN:\n","            self.modules.wav2vec2.model.config.apply_spec_augment = False\n","            self.per_metrics = self.hparams.per_stats()\n","            self.mpd_metrics = MpdStats()\n","\n","    def on_stage_end(self, stage, stage_loss, epoch):\n","        \"\"\"Gets called at the end of a epoch.\"\"\"\n","        if stage == sb.Stage.TRAIN:\n","            self.train_loss = stage_loss\n","        else:\n","            per = self.per_metrics.summarize(\"error_rate\")\n","            mpd_f1 = self.mpd_metrics.summarize(\"mpd_f1\")\n","\n","        if stage == sb.Stage.VALID:\n","\n","            self.hparams.train_logger.log_stats(\n","                stats_meta={\n","                    \"epoch\": epoch,\n","                    \"lr_adam\": self.adam_optimizer.param_groups[0][\"lr\"],\n","                    \"lr_wav2vec\": self.wav2vec_optimizer.param_groups[0][\"lr\"],\n","                },\n","                train_stats={\"loss\": self.train_loss},\n","                valid_stats={\n","                    \"loss\": stage_loss,\n","                    \"ctc_loss\": self.ctc_metrics.summarize(\"average\"),\n","                    \"PER\": per,\n","                    \"mpd_f1\": mpd_f1\n","                },\n","            )\n","            self.checkpointer.save_and_keep_only(\n","                meta={\"PER\": per, \"mpd_f1\": mpd_f1}, min_keys=[\"PER\"], max_keys=[\"mpd_f1\"]\n","            )\n","\n","        if stage == sb.Stage.TEST:\n","            self.hparams.train_logger.log_stats(\n","                stats_meta={\"Epoch loaded\": 0},\n","                test_stats={\"loss\": stage_loss, \"PER\": per, \"mpd_f1\": mpd_f1},\n","            )\n","            with open(self.hparams.wer_file, \"w\") as w:\n","                w.write(\"CTC loss stats:\\n\")\n","                self.ctc_metrics.write_stats(w)\n","                w.write(\"\\nPER stats:\\n\")\n","                self.per_metrics.write_stats(w)\n","                print(\n","                    \"CTC and PER stats written to file\",\n","                    self.hparams.wer_file,\n","                )\n","            with open(self.hparams.mpd_file, \"w\") as m:\n","                m.write(\"MPD results and stats:\\n\")\n","                self.mpd_metrics.write_stats(m)\n","                print(\n","                    \"MPD results and stats written to file\",\n","                    self.hparams.mpd_file,\n","                )\n","\n","\n","def dataio_prep(hparams):\n","    \"\"\"This function prepares the datasets to be used in the brain class.\n","    It also defines the data processing pipeline through user-defined functions.\"\"\"\n","    data_folder = hparams[\"data_folder_save\"]\n","    # 1. Declarations:\n","    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n","        csv_path=hparams[\"test_annotation\"],\n","        replacements={\"data_root\": data_folder},\n","    )\n","    test_data = test_data.filtered_sorted(sort_key=\"duration\")\n","\n","    datasets = [test_data]\n","    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n","\n","    # 2. Define audio pipeline:\n","    @sb.utils.data_pipeline.takes(\"wav\")\n","    @sb.utils.data_pipeline.provides(\"sig\")\n","    def audio_pipeline(wav):\n","        # sig = sb.dataio.dataio.read_audio(wav)\n","        # # sample rate change to 16000, e,g, using librosa\n","        # sig = torch.Tensor(librosa.core.load(wav, hparams[\"sample_rate\"])[0])\n","        # Use wav2vec processor to do normalization\n","        sig = hparams[\"wav2vec2\"].feature_extractor(\n","            librosa.core.load(wav, sr=hparams[\"sample_rate\"])[0],\n","            sampling_rate=hparams[\"sample_rate\"],\n","        ).input_values[0]\n","        sig = torch.Tensor(sig)\n","        return sig\n","\n","    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n","\n","    # 3. Define text pipeline:\n","    @sb.utils.data_pipeline.takes(\"perceived_train_target\", \"canonical_aligned\", \"perceived_aligned\")\n","    @sb.utils.data_pipeline.provides(\n","        \"phn_list_target\",\n","        \"phn_encoded_list_target\",\n","        \"phn_encoded_target\",\n","        \"phn_list_canonical\",\n","        \"phn_encoded_list_canonical\",\n","        \"phn_encoded_canonical\",\n","        \"phn_list_perceived\",\n","        \"phn_encoded_list_perceived\",\n","        \"phn_encoded_perceived\",\n","    )\n","    def text_pipeline_test(target, canonical, perceived):\n","        phn_list_target = target.strip().split()\n","        yield phn_list_target\n","        phn_encoded_list_target = label_encoder.encode_sequence(phn_list_target)\n","        yield phn_encoded_list_target\n","        phn_encoded_target = torch.LongTensor(phn_encoded_list_target)\n","        yield phn_encoded_target\n","        phn_list_canonical = canonical.strip().split()\n","        yield phn_list_canonical\n","        phn_encoded_list_canonical = label_encoder.encode_sequence(phn_list_canonical)\n","        yield phn_encoded_list_canonical\n","        phn_encoded_canonical = torch.LongTensor(phn_encoded_list_canonical)\n","        yield phn_encoded_canonical\n","        phn_list_perceived = perceived.strip().split()\n","        yield phn_list_perceived\n","        phn_encoded_list_perceived = label_encoder.encode_sequence(phn_list_perceived)\n","        yield phn_encoded_list_perceived\n","        phn_encoded_perceived = torch.LongTensor(phn_encoded_list_perceived)\n","        yield phn_encoded_perceived\n","\n","    sb.dataio.dataset.add_dynamic_item([test_data], text_pipeline_test)\n","\n","    # 3. Fit encoder:\n","    # Load the label encoder\n","    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n","    label_encoder.load(lab_enc_file)\n","\n","    # 4. Set output:\n","    sb.dataio.dataset.set_output_keys(\n","        [test_data],\n","        [\"id\", \"sig\", \"phn_encoded_target\", \"phn_encoded_canonical\", \"phn_encoded_perceived\"],\n","    )\n","\n","    return test_data, label_encoder\n","\n","\n"]},{"cell_type":"code","source":["\n","# CLI:\n","hparams_file = '/content/drive/MyDrive/CS5647_Project/hparams/evaluate.yaml'\n","\n","# Load hyperparameters file with command-line overrides\n","with open(hparams_file) as fin:\n","    hparams = load_hyperpyyaml(fin)\n","\n","\n","# Create experiment directory\n","sb.create_experiment_directory(\n","    experiment_directory=hparams[\"output_folder\"],\n","    hyperparams_to_save=hparams_file,\n",")\n","\n","# Load hyperparameters file with command-line overrides\n","with open(hparams_file) as fin:\n","    hparams = load_hyperpyyaml(fin)\n","\n","\n","# Create experiment directory\n","sb.create_experiment_directory(\n","    experiment_directory=hparams[\"output_folder\"],\n","    hyperparams_to_save=hparams_file\n",")\n","\n","# Dataset IO prep: creating Dataset objects and proper encodings for phones\n","test_data, label_encoder = dataio_prep(hparams)\n","\n"],"metadata":{"id":"XnI1Kz09XVlf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697714515802,"user_tz":-480,"elapsed":25327,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"b6c24a14-7ff8-468f-f880-6bff48cc7457"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n","WARNING:speechbrain.lobes.models.huggingface_wav2vec:speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n"]},{"output_type":"stream","name":"stdout","text":["speechbrain.core - Beginning experiment!\n","speechbrain.core - Experiment folder: results/wav2vec2-base_ctc/\n","speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n","speechbrain.core - Beginning experiment!\n","speechbrain.core - Experiment folder: results/wav2vec2-base_ctc/\n"]}]},{"cell_type":"code","source":["# Trainer initialization\n","asr_brain = ASR(\n","    modules=hparams[\"modules\"],\n","    hparams=hparams,\n","    checkpointer=hparams[\"checkpointer\"],\n",")\n","asr_brain.label_encoder = label_encoder\n","\n","# Test\n","asr_brain.evaluate(\n","    test_data,\n","    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n","    min_key=\"PER\"\n",")"],"metadata":{"id":"Mv-9fQATDBsg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697714702608,"user_tz":-480,"elapsed":186825,"user":{"displayName":"lalitha ravi","userId":"00357164084518936800"}},"outputId":"8069b211-c0c1-4a4f-f471-0c5ef2cf5b82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["speechbrain.core - 90.6M trainable parameters in ASR\n","speechbrain.utils.checkpoints - Loading a checkpoint from results/wav2vec2-base_ctc/save/CKPT+2023-10-19+00-55-18+00\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["numexpr.utils - NumExpr defaulting to 8 threads.\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/300 [00:22<?, ?it/s]/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/speechbrain/dataio/encoder.py:722: UserWarning: CTCTextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n","  warnings.warn(\n","100%|██████████| 300/300 [02:57<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["speechbrain.utils.train_logger - Epoch loaded: 0 - test loss: 7.20e-01, test PER: 19.58, test mpd_f1: 3.75e-01\n","CTC and PER stats written to file results/wav2vec2-base_ctc//wer.txt\n","MPD results and stats written to file results/wav2vec2-base_ctc//mpd.txt\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7197456203401091"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"mkPYTbsFqo7G"},"execution_count":null,"outputs":[]}]}